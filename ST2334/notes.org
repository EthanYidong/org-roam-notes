:PROPERTIES:
:ID:       e43586e7-aae2-4bca-aae7-56bec7b36ef0
:END:
#+title: ST2334 Notes
#+filetags: :ST2334:

- links :: [[id:ec7952bd-2932-43a3-98de-69f151c97505][ST2334]]

* Statistical Experiment
:PROPERTIES:
:ID:       66f7bb17-6edb-46ee-a6b7-0f26e5712a09
:END:
- Any procedure that obtains data
* Sample Space
:PROPERTIES:
:ID:       c5ba33ab-a7a1-4cc3-ad6f-52261a7ec0c9
:END:
- The set (usually denoted by \(S\)) of all possible outcomes of a [[id:66f7bb17-6edb-46ee-a6b7-0f26e5712a09][Statistical Experiment]]
* Sample Point
:PROPERTIES:
:ID:       f9b31ced-5160-4717-8776-e9e4b32ff94c
:END:
- Every outcome (element) in a [[id:c5ba33ab-a7a1-4cc3-ad6f-52261a7ec0c9][Sample Space]]
- The brackets \(\{\}\) notates a set and order does not matter; that is, {1, 2} and {2, 1} are indistinguishable
- The brackets \(()\) notates an ordered tuple and order does matter; that is, (1, 2) and (2, 1) are distinguishable
* Event
:PROPERTIES:
:ID:       2b8713f1-41e0-4967-a8af-5bef33624109
:END:
- Subset of a [[id:c5ba33ab-a7a1-4cc3-ad6f-52261a7ec0c9][Sample Space]]
* Sure Event
:PROPERTIES:
:ID:       f09b3ec0-7c31-42de-a5b6-eef1ea0811fe
:END:
- An [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Event]] which is equal to the [[id:c5ba33ab-a7a1-4cc3-ad6f-52261a7ec0c9][Sample Space]]
* Null Event
:PROPERTIES:
:ID:       b07771ac-3814-43c8-ab73-a36a979c3b7a
:END:
- An [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Event]] which does not contain any [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Points]]
* Event Operations
:PROPERTIES:
:ID:       598efe18-f6c2-41f1-9ca1-21a92a9f94ee
:END:
- Operations of [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Events]]
  - Union: \(A \cup B\)
    - The set of all [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Points]] which are in either A or B
    - Over n events: \(\bigcup_{i=1}^{n}A_i\)
  - Intersection: \(A \cap B\)
    - The set of all [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Points]] which are in both A and B
    - Over n events: \(\bigcap_{i=1}^{n}A_i\)
  - Complement: \(A'\), "A prime"
    - The set of all [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Points]] which are not in A
  - Contain: \(A \subset B\)
    - True if all [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Points]] in A are also in B
    - If \(A \subset B\) and \(B \subset A\) then \(A = B\)
    - Note: \(\subset\) here includes when the sets are equal, strict subsets are denoted by \(\subsetneq\)
  - Equivalent: \(A = B\)
    - If every [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Point]] in A are in B, and every [[id:f9b31ced-5160-4717-8776-e9e4b32ff94c][Sample Point]] in B are in A
* Event Operation Properties
:PROPERTIES:
:ID:       8b7d6979-f0ea-4e0f-b85c-df2b3d1ba407
:END:
- Properties of [[id:598efe18-f6c2-41f1-9ca1-21a92a9f94ee][Event Operations]]
  - \(A \cap A' = \emptyset\)
  - \(A \cap \emptyset\) = \emptyset\)
  - \(A \cup A' = S\)
  - \((A')' = A\)
  - \(A\cup(B\cap C) = (A\cup B)\cap(A\cup C)\)
  - \(A\cap(B\cup C) = (A\cap B)\cup(A\cap C)\)
  - \(A\cup B = A \cup (B \cap A')\)
  - \(A = (A \cap B) \cup (A \cap B')\)
  - \(A' \cap B' = (A \cup B)'\) (Extends to more than 2)
  - \(A' \cup B' = (A \cap B)'\) (Extends to more than 2)
* Mutually Exclusive
:PROPERTIES:
:ID:       2e60818e-a6a7-4d04-a74d-d23d44570549
:END:
- Two [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Events]] are mutually exclusive if \(A \cap B\) is equal to \(\emptyset\)
* Counting Methods
:PROPERTIES:
:ID:       6aa1cd9f-fd99-489d-8dc7-eda6533bbc91
:END:
- How many ways for a situation to happen
* Multiplication Principle
:PROPERTIES:
:ID:       4ddfdea2-f063-4a59-8068-dcd98f0ddb86
:END:
- Principle of [[id:6aa1cd9f-fd99-489d-8dc7-eda6533bbc91][Counting]]
- If experiments are performed sequentially, then the number of outcomes of the overall experiment is equal to the product of the number of outcomes of the individual experiments
* Addition Principle
:PROPERTIES:
:ID:       90b31528-500e-45cc-8933-92203b1915f3
:END:
- Principle of [[id:6aa1cd9f-fd99-489d-8dc7-eda6533bbc91][Counting]]
- If an experiment can be carried out with different procedures, then the number of outcomes of the overall experiment is equal to the sum of the number of outcomes of the individual procedures
* Permutations
:PROPERTIES:
:ID:       e7066e9c-c24e-49f4-8173-32ea9f2bbab4
:END:
- Method of [[id:6aa1cd9f-fd99-489d-8dc7-eda6533bbc91][Counting]]
- Number of ways to choose \(n\) objects out of \(r\) objects, including ordering
- \(P_n^r = \frac{n!}{(n-r)!}\)
* Combinations
:PROPERTIES:
:ID:       9d4ab2df-003b-4f3a-8198-1203b69692d7
:END:
- Method of [[id:6aa1cd9f-fd99-489d-8dc7-eda6533bbc91][Counting]]
- Number of ways to choose \(n\) objects out of \(r\) objects, not caring about order
- \(C_n^r = \frac{n!}{(n-r)! \times (r)!}\)
* Probability
:PROPERTIES:
:ID:       7b143707-19ff-4cb9-95a5-c3ad210297d8
:END:
- How likely an [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Event]] will occur
- Probability of A to occur: \(P(A)\)
* Relative Frequency
:PROPERTIES:
:ID:       9c3a91ed-ec3c-488b-a7d7-c6a56f7f1615
:END:
- One interpretation of [[id:7b143707-19ff-4cb9-95a5-c3ad210297d8][Probability]]
- Repeat an experiment E \(n\) times
- Let the event A occur \(n_A\) times
- The relative frequency \(f_A\) of an event A is equal to \(\frac{n_A}{n}\)
- When n approaches infinity, the relative frequency approaches \(P(A)\)
- Properties of \(f_A\)
  - \(0 \leq f_A \leq 1\)
  - \(f_A = 1\) if A occurs in every repetition
  - If A and B are mutually exclusive then \(f_{A\cup B} = f_A + f_B\)
* Basic Properties of Probability
:PROPERTIES:
:ID:       cedcb399-0696-46d2-96d0-d78e72b629d7
:END:
- Propositions about [[id:7b143707-19ff-4cb9-95a5-c3ad210297d8][Probability]]
  - \(P(\emptyset) = 0\)
  - \(P(S) = 1\)
  - \(0\leq P(A) \leq 1\)
  - If A and B are mutually exclusive, then \(P(A\cup B) = P(A) + P(B)\)
    - This extends to any arbitrary number of mutually exclusive [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Events]]
  - \(P(A')=1-P(A)\)
  - \(P(A) = P(A \cap B) + P(A \cap B')\)
  - \(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)
  - If \(A \subset B\) then \(P(A) \leq P(B)\)

* Finite Sample Space, Equal Outcomes
:PROPERTIES:
:ID:       173c2fa8-667b-4cde-b7fe-07450267724a
:END:
- Let the sample space be \(S = {a_1, a_2, ..., a_k}\)
- Assume that \(P(a_1) = P(a_2) = ... = P(a_k)\)
- Then \(P(A)\) = \(\frac{\text{Number of sample points in A}}{\text{Number of sample points in S}}\)

* Conditional Probability
:PROPERTIES:
:ID:       f4f63c08-6447-4f2e-8285-779b35072f4c
:END:
- Find the [[id:7b143707-19ff-4cb9-95a5-c3ad210297d8][Probability]] of an [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Event]] B, but only when we know that another event A has occured
- Denoted as \(P(B \mid A)\), the mid line can be read as "given"
- \(P(B \mid A) = \frac{P(A \cap B)}{P(A)}\)
  - This formula essentially restricts the sample space to the event \(A\)
- We also have \(P(A \mid B) = \frac{P(A)P(B \mid A)}{P(B)}\)
  - This essentially reframes the previous formula using the multiplication rule
* Independence
:PROPERTIES:
:ID:       6172717b-8761-40ed-a989-0f55eb10bcfa
:END:
- Two [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Events]] are independent iff \(P(A\cap B) = P(A)P(B)\)
- This implies that \(P(B \mid A) = P(B)\). If \(B \neq 0\), then two events are independent iff \(P(B \mid A) = 0\) (and vice versa)
- This is denoted as \(A \perp B\)
- Properties of Independent \(X\) and \(Y\)
  - P(X \in A; Y \in B) = P(X \in A) \times P(Y \in B)
  - f(X) and g(Y) are independent for any \(f\) and \(g\)
  - The [[id:f65e3581-f113-47f4-b98b-9317083def60][Conditional Distribution]] of Y, \(f(y | x)\) is equal to the [[id:64af4f21-1f96-4dc1-b0fb-cc5b5a40bdf0][Marginal Probability Distribution]] \(f(y)\)
* Mutually Exclusive
:PROPERTIES:
:ID:       79b01f9a-9ea6-4148-b03e-4b8923181368
:END:
- Two [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Events]] are mutually exclusive iff \(P(A \cap B) = 0\)
- "A and B will never happen at the same time"

* Partition
:PROPERTIES:
:ID:       e7d7332e-0605-4c6a-a062-4987a9b9595c
:END:
- If [[id:2b8713f1-41e0-4967-a8af-5bef33624109][Events]] \(A_1, A_2, A_3, ..., A_n\) are mutually exclusive and \(\cup_{k=1}^{n}A_1\) is equal to the [[id:c5ba33ab-a7a1-4cc3-ad6f-52261a7ec0c9][Sample Space]] \(S\), then \(A_1, A_2, A_3,...,A_n\) is said to be a partition of \(S\).
* The Law of Total [[id:7b143707-19ff-4cb9-95a5-c3ad210297d8][Probability]]
:PROPERTIES:
:ID:       02ce18bb-5c8b-4ed3-aa73-6321bcf6b215
:END:
- If we have a partition \(A_1, A_2, A_3, ..., A_n\), then \(P(B) = \sum_{i=1}^{n}P(B \cap A_i) = \sum_{i=1}^{n}P(A_i)P(B \mid A_i)\)
* Bayes' Theorem
:PROPERTIES:
:ID:       a30eb7b7-e4af-4779-ab4a-5831f89bc095
:END:
- If we have a partition \(A_1, A_2, A_3, ..., A_n\), then \(P(A_k | B)= \frac{P(A_k)P(B\mid A_k)}{\sum_{i=1}^{n}P(A_i)P(B \mid A_i)}\)
- Special case: \(P(A|B)=\frac{P(A)P(B|A)}{P(A)P(B|A)+P(A')P(B|A')}\)
* Random Variables
:PROPERTIES:
:ID:       d5961102-6352-4a14-957a-1928b891b7e3
:END:
- Let \(S\) be the sample space for the outcomes of an experiment
- A function \(X\), which assignes a real number to every element of \(S\) is called a random variable
- Examples:
  - Let \(S={HH, HT, TH, TT}\)
  - This is the sample space for the experiment of flipping two coins
  - Define the random variable \(X\) "the number of heads flipped"
  - \(X(HH) = 2\)
- Uppercase letters denote the random variables themselves
- Lowercase letters denote the specific values from an experiment
** Probability with Random Variables
:PROPERTIES:
:ID:       183ffea3-b9ae-40e7-a1d1-331707c7379b
:END:
- \(P(X = x) = P({s \in S : X(s) = x})\)
- \(P(X \in A) = P({s \in S : X(s) \in A})\)
** Discrete Random Variables
:PROPERTIES:
:ID:       11246aed-52b0-4c98-b9f3-1337548e0502
:END:
- A discrete random variable has a range \(R_x\) which is finite or countable infinite.
- In this case, \(P(X = x)\) is defined and positive when x is in the range of the discrete random variable
** Continuous Random Variables
:PROPERTIES:
:ID:       fe324f54-728a-47e8-8777-1ff360b0457b
:END:
- A continuous random variable has a range \(R_x\) which is an interval or a collection of intervals
* Probability Distributions
:PROPERTIES:
:ID:       ef1c9700-1ee3-44d4-86bd-890b5cd912b9
:ROAM_ALIASES: "Probability Density Function" "Probabiltity Mass Function"
:END:
- For [[id:11246aed-52b0-4c98-b9f3-1337548e0502][Discrete Random Variables]]:
  - Let \(f(x_i) = P(X = x_i)\) for \(x_i \in R_x\) and \(f(x_i) = 0\) otherwise
  - \(f(x)\) is the probability function or probability mass function.
  - The collection of pairs \((x_i, f(x_i))\) is called the probability distrubtion of \(X\).
  - This function will satisfy:
    - \(f(x_i) \geq 0\) for all \(x_i \in R_x\)
    - \(f(x) = 0\) for all \(x \notin R_x\)
    - \(\sum^{\infty}_{i=1}{f(x_i)}=1\)
- For [[id:fe324f54-728a-47e8-8777-1ff360b0457b][Continuous Random Variables]]:
  - For any \(x \in \mathbb{R}\), we have \(P(X=x)=0\)
  - The probability function or probability density function is defined to quantify the probability that \(X\) is in a certain range.
  - Denote this p.d.f. by f(x).
  - This function will satisfy:
    - \(f(x) \geq 0\) for all \(x \in R_x\), and \(f(x) = 0\) otherwise
    - \(\int_{R_x} f(x)dx=1\)
    - For any a and b such that \(a \leq b\), \(P(a \leq X \leq b)=\int_{a}^{b}f(x)dx\)
* Cumulative Distributions
:PROPERTIES:
:ID:       cc1d3ee7-44ab-4d3e-b450-7805be27b50e
:ROAM_ALIASES: CDF
:END:
- We define the cumulative distribution function (c.d.f) as \(F(x) = P(X \leq x)\).
  - This applies to both discrete and continuous [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variables]]
- Using this, we can find the value \(P(a \leq X \leq b) = P(X \leq b) - P(X < a) = F(b) - F(a-)\) where \(a-\) is the largest value in \(R_x\) that is less than \(a\)
- For [[id:11246aed-52b0-4c98-b9f3-1337548e0502][Discrete Random Variables]]:
  - \(F(x)\) is just the sum of all \(P(X = k)\) such that \(k \leq x\)
- For [[id:fe324f54-728a-47e8-8777-1ff360b0457b][Continuous Random Variables]]:
  - \(F(x) = \int_{-\infty}^{x}f(t)dt\)
- This function satisfies:
  - \(0 \leq F(x) \leq 1\)
* Expectation of [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variables]]
:PROPERTIES:
:ID:       c49ef5b5-51fb-434b-a145-fbd8eebabae0
:END:
- "Mean" or "expected value" of \(X\)
- Denoted as \(E(X)\) or \(\mu_X\)
- For [[id:11246aed-52b0-4c98-b9f3-1337548e0502][Discrete Random Variables]]:
  - \(E(X) = \sum_{x_i \in R_X} x_i f(x_i)\)
- For [[id:fe324f54-728a-47e8-8777-1ff360b0457b][Continuous Random Variables]]:
  - \(E(X) = \int_{-\infty}^\infty xf(x)dx\)
- Satisfies:
  - \(E(aX+b)=aE(X)+b\)
  - \(E(X+Y)=E(X)+E(Y)\)
  - If g is an arbitrary function, then \(E[g(X)]=\sum_{x\in R_X}g(x)f(x)\) for discrete random variables
    - \(E[g(X)] = \int_{R_X}g(x)f(x)dx\) for continuous random variables
* Variance of [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variables]]
:PROPERTIES:
:ID:       ab8cb1c3-a402-4c15-9317-b327447a3d5a
:END:
- Denoted as \(\sigma_X^2=V(X)=E(X-\mu_X)^2\),
- Properties:
  - \(V(X) \geq 0\)
  - \(V(X) = 0\) iff \(P(X=E(X)) = 1\), i.e. \(X\) is constant
  - \(V(aX+b)=a^2 V(X)\)
  - \(V(X)=E(X^2)-[E(X)]^2\)
  - \(\sigma(X) = \sqrt{V(X)}\)

* Joint Distributions
:PROPERTIES:
:ID:       c89a77a2-3af0-42ec-b6ba-d371bc47439a
:END:
- This is what happens when we are interested in multiple random variables at once
- A two-dimentional random vector or a two-dimentional random variable is denoted as (X, Y) where X and Y are [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variables]]
  - In this case, the range space is the range of all possible ordered pair outputs (x, y) from applying the function to all possible outcomes in the sample space.
  - Note that this is not the cartesian product of the ranges of the individual variables, as some outcomes are unattainable (e.x. dice roll is a multiple of 4 and is odd)
- This definition works for an arbitrary number of variables
- A N-dimentional RV is a [[id:11246aed-52b0-4c98-b9f3-1337548e0502][Discrete Random Variable]] iff the range is countable
- It is a [[id:fe324f54-728a-47e8-8777-1ff360b0457b][Continuous Random Variable]] if it can assume any value in some defined range of the space \(\mathbb{R}^N\)
- If all components are descrete, then it is descrete
- If all components are continuous, then it is continuous
- In other cases, unhandled for now
** Joint Probability Mass Function
:PROPERTIES:
:ID:       1989137d-e58f-4685-9e4b-b8ac9760b3c2
:END:
- For [[id:11246aed-52b0-4c98-b9f3-1337548e0502][Discrete Random Variables]]:
  - Defined as \(f_{X,Y}(x,y)=P(X=x, Y=y)\)
  - Satisfies:
    - \(f(x,y) \geq 0\) for any \((x,y)\)
    - \(f(x,y) = 0\) for any \((x,y) \notin R_{X,Y}\)
    - \(\sum_{i=1}^\infty  \sum_{j=1}^\infty f(x_i,y_j)=1\)
- For [[id:fe324f54-728a-47e8-8777-1ff360b0457b][Continuous Random Variables]]:
  - Defined as \(f_{X,Y}(x,y)\) such that \(P((X,Y)\in D)= \int \int_{(x,y)\in D} f(x,y)dy dx\)
    - \(P(a \leq X \leq b, c \leq Y \leq d) = \int_a^b \int_c^d f(x,y)\)
  - Satisfies:
    - \(f(x,y) \geq 0\) for any \((x,y)\)
    - \(f(x,y) = 0\) for any \((x,y) \notin R_{X,Y}\)
    - \(\int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) dy dx =1\)
** Joint Distibutions of [[id:6172717b-8761-40ed-a989-0f55eb10bcfa][Independent]] Random Variables
:PROPERTIES:
:ID:       dbef14ae-c127-42b2-ac71-fe79db1ad2ce
:END:
- Iff we have a joint distribution of [[id:6172717b-8761-40ed-a989-0f55eb10bcfa][Independent]] Random Variables, then we have \(R_{X,Y}={(x,y)x \in R_X; y \in R_Y}=R_X \times R_Y\)
** Expectation for Joint Distributions
:PROPERTIES:
:ID:       43cf4c4d-822e-4221-94d0-1bbc0c9b1e52
:END:
- Assuming we have [[id:1989137d-e58f-4685-9e4b-b8ac9760b3c2][Joint Probability Mass Function]] f(x,y), then:
  - \(E(g(X,Y)) = \sum_x \sum_y g(x,y) f(x,y)\) for discrete distributions
  - \(E(g(X,Y)) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f(x,y) dy dx\) for continuous distributions
** Covariance for Joint Distributions
:PROPERTIES:
:ID:       7cac8039-e815-4565-892f-1cc785e861b6
:END:
- If we let \(g(X,Y) = (X - E(X))(Y - E(Y))= (X- \mu_X)(Y - \mu_Y)\), then the covariance is given by \(E(g(X,Y))\) ([[id:43cf4c4d-822e-4221-94d0-1bbc0c9b1e52][Expectation for Joint Distributions]])
- Properties:
  - \(cov(X, Y) = E(XY) - E(X)E(Y)\)
  - If X and Y are [[id:6172717b-8761-40ed-a989-0f55eb10bcfa][Independent]], then \(cov(X,Y) = 0\), but we do not have the converse.
  - \(cov(aX+b, cY+d) = ac \times cov(X,Y)\)
  - \(V(aX + bY) = a^2 V(X) + b^2 V(Y) + 2ab \times cov(X,Y)\) ([[id:ab8cb1c3-a402-4c15-9317-b327447a3d5a][Variance of Random Variables]])

* Marginal Probability Distribution
:PROPERTIES:
:ID:       64af4f21-1f96-4dc1-b0fb-cc5b5a40bdf0
:END:
- If we have a 2-dimentional [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variables]] with a [[id:1989137d-e58f-4685-9e4b-b8ac9760b3c2][Joint Probability Mass Function]] \(f(x, y)\), then the marginal distribution of X is
  - \(f(x) = \sum_{y}f(x,y)\) for discrete y
  - \(f(x) = \int_{-\infty}^{\infty}f(x,y)dy\)
- This is also how we get the marginal distribution of Y
- \(f\) is a probability function and thus follows the properties of [[id:ef1c9700-1ee3-44d4-86bd-890b5cd912b9][Probability Distributions]]

* Conditional Distribution
:PROPERTIES:
:ID:       f65e3581-f113-47f4-b98b-9317083def60
:END:
- Given the [[id:1989137d-e58f-4685-9e4b-b8ac9760b3c2][Joint Probability Mass Function]] \(f(x,y)\) and the [[id:64af4f21-1f96-4dc1-b0fb-cc5b5a40bdf0][Marginal Probability Distribution]] \(f(x)\), we get
- \(f(y|x)=\frac{f(x, y)}{f(x)}\)
- This is the probability distribution of \(Y\) for some given value \(x\)
- \(f\) is a probability function and thus follows the properties of [[id:ef1c9700-1ee3-44d4-86bd-890b5cd912b9][Probability Distributions]]
* Special Distributions
:PROPERTIES:
:ID:       5b9dc76e-1144-4d31-8e32-41395e082eb8
:END:
** Discrete Uniform Distribution
:PROPERTIES:
:ID:       60c19a91-b5f0-4074-ab3e-b58211481c9a
:END:
- If a [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variable]] X takes values \(x_1, x_2, ..., x_k\) with equal probability \(\frac{1}{k}\), then X follows a discrete uniform distribution
- The [[id:ef1c9700-1ee3-44d4-86bd-890b5cd912b9][Probability Density Function]] is given by \(f(x)=\frac{1}{k}\) for \(x= x_1, x_2, ..., x_k\) and 0 otherwise
- We need k to be finite for this definition to work.
- Properties:
  - \(E(X) = \frac{1}{k} \sum_{i=1}^k x_i\)
  - \(V(x) = \frac{1}{k} \sum_{i=1}^k (x_i^2) - E(X)^2\)
** Bernoulli Trial
:PROPERTIES:
:ID:       eb8c19c4-3d36-4b42-8769-ed0ba1022a6a
:END:
- A Bernoulli Trial is a [[id:66f7bb17-6edb-46ee-a6b7-0f26e5712a09][Statistical Experiment]] with only two possible outcomes
- We call them 1 "success" and "0" failure
** Bernoulli Random Variable
:PROPERTIES:
:ID:       eaf882ff-2c9a-424a-a54f-a2d7072c2402
:END:
- A Bernoulli Random Variable is a [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variable]] X where X = the outcome of a [[id:eb8c19c4-3d36-4b42-8769-ed0ba1022a6a][Bernoulli Trial]].
- Let \(p\) be the probability of success
- The [[id:ef1c9700-1ee3-44d4-86bd-890b5cd912b9][Probability Density Function]] is given by \(\begin{cases}p & x = 1 \\ (1-p) & x = 0\end{cases}\), and 0 otherwise
- Also equal to \(p^x(1-p)^{1-x}\) for \(x=0,1\)
- Can be donated as \(X \sim \text{Bernoulli}(p)\)
- Properties:
  - \(E(X) = p\)
  - \(V(X) = p(1-p)\)
** Bernoulli Process
:PROPERTIES:
:ID:       ca32470f-3844-47bd-bd2d-dc4c0e5ccf84
:END:
- A sequence formed by performing many identical and independent [[id:eb8c19c4-3d36-4b42-8769-ed0ba1022a6a][Bernoulli Trials]]
- Denoted by \(X_1, X_2, X_3, ...\)
** Binomial Distribution
:PROPERTIES:
:ID:       5a71c997-228a-4e59-a2fb-6ca86007d36f
:END:
- A binomial [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variable]] counts the number of successes in \(n\) trials in a [[id:ca32470f-3844-47bd-bd2d-dc4c0e5ccf84][Bernoulli Process]] with probability \(p\) of success
- We say X has a binomial distribution and denote \(X \sim B(n,p)\)
- Properties:
  - \(P(X=x) = {n \choose x} p^x (1-p)^{n-x}\)
  - \(E(X) = np\)
  - \(V(X) = np(1-p)\)
** Negative Binomial Distribution
:PROPERTIES:
:ID:       e921bb31-0f2f-406b-81e5-4fa6b7424574
:END:
- A binomial [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variable]] counts the number of trials needed to get \(k\) successes in a [[id:ca32470f-3844-47bd-bd2d-dc4c0e5ccf84][Bernoulli Process]] with probability \(p\) of success
- We say X has a negative binomial distribution \(X \sim NB(k,p\)
- Properties:
  - \(P(X=x) = {{x-1} \choose {k-1}}p^k(1-p)^{x-k}\)
  - \(E(X) = \frac{k}{p}\)
  - \(V(X) = \frac{(1-p)k}{p^2}\)
** Geometric Distribution
:PROPERTIES:
:ID:       2b95e4a0-f28c-4bff-84e4-7cf59de0b5f4
:END:
- A geometric [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variable]] is a special case of the [[id:e921bb31-0f2f-406b-81e5-4fa6b7424574][Negative Binomial Distribution]] which counts the number of trials needed to get 1 success in a [[id:ca32470f-3844-47bd-bd2d-dc4c0e5ccf84][Bernoulli Process]] with probability \(p\) of success
- Properties:
  - \(P(X=x) = (1-p)^{x-1}p\)
  - \(E(X) = \frac{1}{p}\)
  - \(V(X) = \frac{1-p}{p^2}\)
** Poisson Distribution
:PROPERTIES:
:ID:       934104ef-8fc3-4fe7-9afb-4139517e2a55
:END:
- A Poisson [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variable]] is the number of times an event occurs in a set period of continuous time
  - This period is defined such that, given a rate parameter \(\alpha\)
    - The expected number of occurances in an interval of length \(T\) is \(\alpha T\)
    - There are no simultaneous occurances
    - The number of occurances in disjoint time intervals are independent
- We denote \(X \sim \text{Poisson}(\lambda)\), where \(\lambda\) is the expected number of occurences during that set period of time.
  - Also, \(X \sim \text{Poisson} (\alpha T)\)
- Properties:
  - \(P(X=k) = \frac{e^{-\lambda}\lambda^{k}}{k!}\)
  - \(E(X) = \lambda\)
  - \(V(X) = \lambda\)
*** Poisson Approximation of [[id:5a71c997-228a-4e59-a2fb-6ca86007d36f][Binomial Distribution]]
:PROPERTIES:
:ID:       83c48189-3581-4e80-bc89-2471150bd61c
:END:
- Let \(X \sim B(n,p)\)
- If \(n \rightarrow \infty\) and \(p \rightarrow 0\) in such a way that \(\lambda = np\)
- Then, we approximate \(X \sim \text{Poisson}(np)\)
- \(\lim_{p \to 0; n \to \infty} P(X=x) = \frac{e^{-np}(np)^x}{x!}\)
- This is a good approximation when \(n \geq 20\) and \(p \leq 0.05\) or if \(n \geq 100\) and \(np \leq 10\)
* Special [[id:fe324f54-728a-47e8-8777-1ff360b0457b][Continuous]] Distributions
:PROPERTIES:
:ID:       873a174d-6f9e-4326-8b05-71fdf3e19b3e
:END:
** Continuous Uniform Distribution
:PROPERTIES:
:ID:       8ddc46ba-8c7b-41a2-a87b-99751b037804
:END:
- A random variable \(X\) with a constant probability over an interval \((a,b)\)
- Denoted as \(X \sim U(a,b)\)
- Properties:
  - \(f(x) = \frac{1}{b-a}\) for \(x\) between \(a\) and \(b\), and 0 otherwise
  - \(F(x)=\frac{x-a}{b-a}\) for \(a \leq x \leq b\), 0 for \(a > x\) or 1 otherwise
  - \(E(x) = \frac{a+b}{2}\)
  - \(V(X)=\frac{(b-a)^2}{12}\)
** Exponential Distribution
:PROPERTIES:
:ID:       1b06df73-cf5e-4f16-be8f-79fa230ec9d8
:END:
- A distribution parametrized by \(\lambda > 0\) such that:
- Denoted as \(X \sim Exp(\lambda)\)
- Properties:
  - \(f(x)=\lambda e^{-\lambda x}\) for \(x \geq 0\), and 0 otherwise
  - \(F(x)= 1-e^{-\lambda x}\) for \(x > 0\), and 0 otherwise
  - \(E(X) = \frac{1}{\lambda}\)
  - \(V(X)=\frac{1}{\lambda^2}\)
- Can also be seen in the form where \(\mu = \frac{1}{\lambda}\)
- \(P(X > s + t | X > s) = P(X > t)\)
  - Conceptually, this means that the exponential distribution has "no memory"
  - For example, if a bulb's lifetime follows such as distribution, the chance that it lasts for t more seconds after s seconds is the same as sthe chance that a new bulb would last for t seconds
** Normal Distribution
:PROPERTIES:
:ID:       5caa8927-ec1b-4eb4-9989-9eb2ebd70684
:END:
- Denoted as \(X \sim N(\mu, \sigma^2)\)
- Properties:
  - \(f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{(2 \sigma^2)}}\)
  - \(E(X) = \mu\)
  - \(V(X) = \sigma^2\)
- Symmetric about \(x = \mu\) and positive across all real numbers
- Two normal curves with the same \(\sigma^2\) have the same shape but centered at different locations
- Larger \(\sigma\) flattens the curve
- If we have \(Z = \frac{X - \mu}{\sigma}\), then \(Z \sim N(0, 1)\)
  - Using this, \(P(x_1 < X < x_2) = P(z_1 < Z < z_2)\) where \(z_i = \frac{x_i - \mu}{\sigma}\)
- We use \(\phi\) to denote the pdf and \(\Phi\) to denote the cdf of Z.
  - \(\Phi(z) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{z} e^{\frac{-t^2}{2}}dt\)
  - No closed form formula, so we use a table
- Properties of Z:
  - \(P(Z \geq 0) = P(Z \geq 0) = 0.5\)
  - \(\Phi(z) = 1 - \Phi(-z)\)
  - \(-Z \sim N(0,1)\)
- When \(n \to \infty\), p remains constant, the normal distribution can be used to approximate the [[id:5a71c997-228a-4e59-a2fb-6ca86007d36f][Binomial Distribution]]
  - Use when \(np > 5\) and \(n(1-p) > 5\)
  - Given \(X \sim B(n, p)\), we have \(Z \approx \frac{X - E(X)}{\sqrt{V(X)}}\)
* Quantile
:PROPERTIES:
:ID:       470591ea-e5b6-4980-94b6-5ec5ab1dfad7
:END:
- The \(\alpha\)th upper quantile of \(X\) is the number \(x_\alpha\) where \(P(X \geq x_\alpha) = \alpha\)
- In particular, we denote \(z_\alpha\) as the \(\alpha\)th upper quantile of \(Z \sim N(0,1)\) ([[id:5caa8927-ec1b-4eb4-9989-9eb2ebd70684][Normal Distribution]])

* Sampling
:PROPERTIES:
:ID:       d2a95892-8364-4cf7-98bd-0212d5892133
:END:
- Statistical inference is the process of making claims about a population given data from a sample
** Population
:PROPERTIES:
:ID:       35a4e4f1-4045-4aa0-a04e-8105310040ad
:END:
- The population is is the totality of all possible outcomes of a statistical experiment or survey
- Could be finite or infinite
  - Finite: "the monthly income of Singaporeans"
    - Some large finite populations are assumed to be infinite for practical purposes
  - Infinite: "the continuous air pressure in Singapore"
** Random Sample
:PROPERTIES:
:ID:       53296da7-9b2f-491e-a014-0430bbcc4343
:END:
- We want to take a sample of the population which represents the population as a whole
- One method is to take a simple random sample
  - Choosing n members out of a population such that each member has the same probability of being selected
    - * unsure if applies to this module, but usually also implies that each member's selection is independent from all other member's
  - Tends to yield a sample which resembles the population
  - For an infinite population:
    - Let \(X_1, X_2, X_3..., X_n\) be independent random variables with the same distribution as \(X\).
    - Then, \(X_1, ... X_n\) is a random sample of size n from a probability with distribution \(f_X(x)\)
    - The joint probability function \(f_{X_1, ..., X_N}(x_1, x_2, ..., x_n) = f_{X_1}(x_1) \times ... \times f_{X_n}(x_n)\)
** Statistic
:PROPERTIES:
:ID:       be899b9e-95a7-49f7-a25b-e62bb7b9eb01
:END:
- A function computed using a [[id:53296da7-9b2f-491e-a014-0430bbcc4343][Random Sample]] of n observations is called a statistic
- For example, the sample mean, \(\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i\) is a statistic
- The realization of a statistic is the value obtained after plugging in observed values.
- Other examples:
  - Sample variance: \(S^2=\frac{1}{n-1} \sum_{i=1}^{n}(X_i-\bar{X})^2\)
- Variables are also [[id:d5961102-6352-4a14-957a-1928b891b7e3][Random Variables]], so we may also consider its probability distribution, also known as the [[id:6357e52c-0688-4e89-8431-266349386a6c][Sampling Distribution]]
- Statistics must be computable using the sample, and not use any unknown variables such as the true population mean
** Sampling Distribution
:PROPERTIES:
:ID:       6357e52c-0688-4e89-8431-266349386a6c
:END:
- Standard Error: The standard deviation of a sampling distribution. Commonly used on the sampling mean, denoted as \(\sigma_{\bar{X}}\), the "standarad error of" \(\bar{X}\)
  - Describes how \(\bar{X}\) varies from sample to sample
- Theorems:
  - For a random sample of an infinite population with mean \(\mu_X\) and variance \(\sigma_X^2\), the sampling distribution of the sample mean has mean \(\mu_X\) and variance \(\frac{\sigma_X^2}{n}\)
  - Given n independent random variables with the same mean and variance, \(P(|\bar{X}-\mu|>\epsilon) \to 0\) as \(n \to \infty\)
    - In other words, the probability that the difference between the sample mean and the population mean is larger than some threshold approaches 0 as the sample size gets larger
  - Central limit theorem: \(\bar{X}\) is approximately normal
    - Formally, \(\frac{\bar{X}-\mu}{\sigma / \sqrt{n}} \to Z \sim N(0, 1)\). In other words, \(\bar{X} \sim N(\mu, \sigma^2/n)\)
    - If the population is normal to begin with, this holds even for small \(n\)
    - If the population is symmetric, normality may start to appear at around 15 or 20 samples
    - If the population is moderately skewed, normality may appear at around 30 to 50 samples
    - For extremely skewed data such as financial data where many entries are 0, some are very small, some are very large, normality may not appear even at 1000 samples
** Chi-Squared Distribution
:PROPERTIES:
:ID:       67a9990e-4e8a-4702-8a3b-f28b079a7cbf
:END:
- Let \(Z\) be a normal random variable. A random variable with the same distribution as \(Z^2\) is called a \(\chi^2\) random variable with one degree of freedom.
- If we have \(Z_1, Z_2, ..., Z_n\), all normal random variables, then a random variable with the same distribution as \(Z_1^2+ ... + Z_n^2\) is called a \(\chi^2\) random variable with \(n\) degrees of freedom
- Denote this as \(\chi^2(n)\)
- Properties:
  - If \(Y \sim \chi^2(n)\): \(E(Y)=n\) and \(var(Y)=2n\)
  - For large \(n\), \(\chi^2(n)\) is approximately \(N(n, 2n)\) by the central limit theorem
  - If \(Y_1 and Y_2\) are \(\chi^2\) random variables with \(n\) and \(m\) degrees of freedom respectively, then \(Y_1 + Y_2\) is a \(\chi^2\) random variable with \(m+n\) degrees of freedom
  - \(\chi^2\) distribution is a family of curves defined by \(n\), all with a long right tail.
- Define \(\chi^2(n, \alpha)\) such that \(P(Y > \chi^2(n, \alpha)) = \alpha\)
- Consider the random variable \(\frac{(n-1)S^2}{\sigma^2}\) when \(X_i \sim N(\mu, \sigma^2)\) for all \(i\)
  - This distribution has a [[id:67a9990e-4e8a-4702-8a3b-f28b079a7cbf][Chi-Squared Distribution]] with \(n-1\) degrees of freedom.
** T-Distribution
:PROPERTIES:
:ID:       bd7fb354-8e9f-4a20-bbef-0bcf57cdf45c
:END:
- Suppose \(Z \sim N(0,1)\) and \(U \sim \chi^2(n)\)
- Then, the random variable \(T = \frac{Z}{\sqrt{U/n}}\) is a T-distribution with n degrees of freedom
- Denoted as \(t(n)\)
- Properties:
  - As \(n \to \infty\), the T-distribution approaches the normal distribution \(Z\).
    - For \(n \geq 30\), we can replace \(t(n)\) by \(N(0,1)\)
  - \(E(T) = 0\), \(var(T)=\frac{n}{n-2}\) when \(n > 2\)
  - The graph is symmetric about the vertical axis and looks almost normal
- Define \(t_{n;\alpha}\) such that for \(T \sim t(n)\), \(P(T > t_{n;\alpha}) = \alpha\)
- Consider the random variable \(\frac{X - \mu}{S/\sqrt{n}}\).
  - This distribution follows a t-distribution with \(n-1\) degrees of freedom.
** F-Distribution
:PROPERTIES:
:ID:       c64ebbd0-6fbe-41ea-85cc-2e95d7c40348
:END:
- Suppose \(U \sim \chi^2(m)\) and \(V \sim \chi^2(n)\)
- Then, the random variable \(F = \frac{U/m}{V/n}\) is an F-distribution with \((m,n)\) degrees of freedom.
- Denoted as \(F(m,n)\)
- Properties:
  - \(E(X)=\frac{n}{n-2}\)
  - \(var(X) = \frac{2n^2(m+n-2)}{m(n-2)^2(n-4)}\)
  - \(\frac{1}{F(m,n)} \sim F(n,m)\)
- Define \(F(m,n;\alpha)\) such that for \(F \sim F(m,n)\), \(P(F > F(m,n;\alpha) = \alpha\)
  - \(\frac{1}{F(m,n;1-\alpha)} = F(n,m;\alpha)\)

* Statistical Inference
:PROPERTIES:
:ID:       096a2980-194a-4872-b44e-93bdbd7e732a
:END:
- We want to infer or estimate the population parameters given Sampling [[id:be899b9e-95a7-49f7-a25b-e62bb7b9eb01][Statistic]]
- We use the sampling distribution to gather information about the population parameters
- We will use large = greater than or equal to 30, small = less than 30.
** Point Estimate
:PROPERTIES:
:ID:       3a5525e5-90a1-476b-967e-0b86c4a84da2
:END:
- Guess a single value for the population parameter
- Use an estimator:
  - An estimator is a rule which tells us how to calculate an estimate based on information in a sample
- For example: "Estimate the average for the population by the average for the sample", estimate that \(\mu = \bar{X}\)
  - This is an "unbiased estimator", meaning that \(E(\bar{X}) = \mu\). "On average", the estimator is right
  - Assuming we can apply the central limit theorem
  - \(P(| \bar{X} - \mu | \leq z_{\alpha / 2} \times \frac{\sigma}{\sqrt{n}}) = 1 - \alpha\) where \(P(Z > z_{\alpha})=\alpha\)
  - "With probability \(1-\alpha\), the error \(|\bar{X} - \mu |\) is less than \(E = z_{\alpha/2} \times \frac{\sigma}{\sqrt{n}}\)"
  - We call this quanity \(E\) the maximum error of estimate
  - Given a fixed \(\alpha\) and a maximum \(E\), we can find the minimum \(n\) that produces this relationship
  - [[file:media/estimator_1.png][Estimators]]
*** Unbiased Estimator
:PROPERTIES:
:ID:       37e236a9-17ab-4e6d-ae58-17f3a5029b92
:END:
- If \(\hat{\Theta}\) is an estimator of \(\theta\):
- \(\hat{\Theta}\) is an unbiased estimator of \(\theta\) if \(E(\hat{\Theta})=\theta\)
** Interval Estimate
:PROPERTIES:
:ID:       98754c05-6c5e-45ba-9acd-bfead8ee34f9
:END:
- Guess a range for the population parameter
- We want to find an interval \((a,b)\) such that \(P(a < \mu < b) = 1 -\alpha \). In that case, we call \((a, b)\) the \(1- \alpha\) confidence interval.
- For Case I: if \(\sigma\) is known and the data is normal
  - We know \(P(-z_{\alpha/2} \leq \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \leq z_{\alpha/2}) = 1-\alpha\)
  - So, by rearranging, we get \(P(\bar{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) = 1-\alpha\)
  - This gives us a confidence inverval for estimating \(\mu\)
  - This can also be written as \(\bar{X} \pm E\)
- [[file:media/confidence-interval_1.png][Chart]]
- "If we repeat this procedure of sampling repeatedly, \(1-\alpha\) of the times, our confidence interval will contain the true parameter"

* Comparing Populations
:PROPERTIES:
:ID:       f077aca2-5c90-4c2a-9027-0f4193d68885
:END:
- Sometimes, we want to compare two populations given a sample of each of them
- This occurs when we want to compare outcomes of two possible treatments
  - Independent samples: randomly choose subjects for each treatment
    - Both samples are independent from each other, as are the individuals within each sample
  - Matched pair samples: pair the subjects (possible by some factor) and randomize between which subject in each pair gets which treatment
    - The observations within a pair are dependent on each other
    - However, observation between pairs are independent
- We want to conduct [[id:096a2980-194a-4872-b44e-93bdbd7e732a][Statistical Inference]] on \(\mu_1 - \mu_2\), the averages from two different samples
  - Assume:
    - Unequal but known variances
    - Two samples are independent
    - Both samples are large or normal
  - \(E(\bar{X} - \bar{Y}) = \mu_1 - \mu_2 = \delta\)
  - Because the two samples are independent, \(var(\bar{X}-\bar{Y}) = \frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}\)
  - Therefore, since the sample distribution is close to normal, we can approximate \(\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}} \approx N(0,1)\)
  - We can therefore construct our confidence interval as describe in [[id:98754c05-6c5e-45ba-9acd-bfead8ee34f9][Interval Estimate]]
- What if variances are unknown?
  - We instead use the sample variances to approximate the population variances
- What if the variances are unknown but equal?
  - We now assume that both samples are small, but the population is normally distributed
  - First, used the "pooled estimator" to estimate \(\sigma^2\) by using results from both samples
  - \(S_p^2 = \frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}\)
  - Then, the random variable \(\frac{(\bar{X} - \bar{Y})-(\mu_1-\mu_2)}{S_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}\) follows a [[id:bd7fb354-8e9f-4a20-bbef-0bcf57cdf45c][T-Distribution]] with \(n_1 + n_2 -2\) degrees of freedom
  - For large samples, we simply replace the t critical values with z critical values
- How about paired data?
  - We have \(n\) pairs \((X_i, Y_i)\)
    - \(X_i\) and \(Y_i\) are dependent
    - \((X_i, Y_i)\) and \((X_j, Y_j)\) are independent if \(i \neq j\)
    - We define \(D_i = X_i - Y_i\), and want to estimate \(\mu_D = \mu_1 - \mu_2\)
  - Now, we have one single random sample of \(D_i\)
  - Thus, we simply apply the techniques for single samples ([[id:98754c05-6c5e-45ba-9acd-bfead8ee34f9][Interval Estimate]])
