:PROPERTIES:
:ID:       3BEC0A0C-17C5-4C68-9937-E44E79DB9C4C
:END:
#+title:CS2106 Notes
#+filetags: :CS2106:
- links :: [[id:539C8BDD-D2EA-4131-8F31-F2C3F0BC3799][CS2106]]

* QNA
:PROPERTIES:
:ID:       e1a1f936-5cb0-4093-a7da-5a375d9a8655
:END:
- KiB = 1024, KB = 1000
- Isn't the OS an overhead? Wouldn't it be better to do without?
  - It is an overhead but it is worth it in almost all cases, sometimes it can actually save on some resources
  - In some MCUs there is no OS because the computer only has one job
- What if I only have one CPU? How can it run so many things?
  - The OS can take over the CPU ("scheduled to run") and transfer control to other processes
  - Context switching
    - Register data is stored into the memory
  - Hardware is designed for interrupts to be possible
- What is system software?
  - Software in the OS that assists and services the other applications running
- How do you know how much space to allocate?
  - Compiler checks which variables you want to store
- Endianness
  - Let a value \(A=0xdeadbeef\)
  - Let's say we want to store this in the memory space: \(0x100, 0x101, 0x102, 0x103\)
  - Little Endian:
    - [[id:0a64e439-8706-4401-ab6a-71577970d7aa][MSB]] is stored in the lowest address
|  Addr |  Val |
|-------+------|
| 0x100 | 0xde |
| 0x101 | 0xad |
| 0x102 | 0xbe |
| 0x103 | 0xef |

  - Big Endian:
    - [[id:6a212478-657e-4121-a5f2-ad26de5cf35e][LSB]] stored in the lowest address
|  Addr |  Val |
|-------+------|
| 0x100 | 0xef |
| 0x101 | 0xbe |
| 0x102 | 0xad |
| 0x103 | 0xde |

* Operating System
:PROPERTIES:
:ID:       D289CD47-38F4-481F-BED1-FEAF25C4D709
:ROAM_ALIASES: OS
:END:

** Why learn OS?
- Become a better programmer
  - Know how your program is interacting with the OS.
- Become a better software engineer
  - Understand the most complicated program
- Become a better computer user
** What is an OS?
- Invented in order to allow the creation of general-purpose computers which were not hardcoded to do their one job
- Abstractly, was developed as something that can take "code" and cause the effects described by the code.
  - A program that controls a program
- Definition
  - "A program that acts as an intermediary between a computer user (AKA the program) and the computer hardware."
  - [[https://en.wikipedia.org/wiki/Operating_system][Wikipedia]]: "An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs."
    - "System software" which comes with the system, and does not interact directly with the user
    - Hardware: physical objects
    - Software: virtual objects
    - "Common services": providing tools that allow interaction with the hardware
      - [[id:FAAB67BF-9DDB-4AC3-AA45-472F439686EB][Client-Server Model]]
** Examples
- PC
  - Windows
  - macOS
  - Linux
  - Solaris
  - FreeBSD
- Mobile
  - Android
  - iOS
- Other
  - XBOX, PlayStation

* Batch [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]
:PROPERTIES:
:ID:       D9F3E442-3F6C-48DF-A404-283C7A15CFBA
:END:
- Run user program (jobs) one at a time

* Multiprogramming
:PROPERTIES:
:ID:       70308734-2797-4277-9DF1-5A145F773AC7
:END:

* Time Sharing [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]
:PROPERTIES:
:ID:       6276534B-2CDD-4F8B-BD8A-73DDEA2C1A31
:END:
- Allows multiple points of interaction using terminals (teletype, AKA TTY)
- Schedule user jobs
  - Illusion of [[id:62A2FCE1-6909-4C5A-8D25-015D1F2FAAFA][concurrency]]
- Memory management / provisioning between jobs
- CPU time, memory, and storage are split between the [[id:CEED7EB1-C9DD-40C6-ABBF-32D3E41FA6F7][users]].
*** Examples
- CTSS at MIT, 1960s
- Multics, 1970s


* [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] Implementations
:PROPERTIES:
:ID:       28C8C09A-0B31-4354-AD0F-FE83226939E9
:END:
** OS For Mainframes
:PROPERTIES:
:ID:       A1AF2D25-EF35-45E0-A085-9487826DD8B7
:END:
*** IBM360
:PROPERTIES:
:ID:       8A913B91-E03C-4348-9AF3-9FE55CA7290D
:END:
- International Business Machines
- Programmed using punchcards
- The [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]:
  - [[id:D9F3E442-3F6C-48DF-A404-283C7A15CFBA][Batch OS]]
  - Still interact with hardware directly
  - Code gains some additional information from the OS

*** Improvements
- CPU is idle when doing I/O, so we should run other tasks
- [[id:70308734-2797-4277-9DF1-5A145F773AC7][Multiprogramming]]
- [[id:6276534B-2CDD-4F8B-BD8A-73DDEA2C1A31][Time Sharing OS]]

** Minicomputer
- A smaller mainframe, adopting the same ideas about the [[id:A1AF2D25-EF35-45E0-A085-9487826DD8B7][OS]]
- Adoption of [[id:C4CA2869-8F42-446C-A25A-570E4765A00C][UNIX]], developed by AT&T

*** Examples
- PDP-11, 1970

* Client-Server Model
:PROPERTIES:
:ID:       FAAB67BF-9DDB-4AC3-AA45-472F439686EB
:END:
- A server acts as an intermediary between a client and another interface.
  - In the case of the OS, the clients are the programs and the interfaces are the hardware.

* Concurrency
:PROPERTIES:
:ID:       62A2FCE1-6909-4C5A-8D25-015D1F2FAAFA
:END:
- Jobs can be "active" at the same time, which means that the CPU can be processing multiple jobs at the same time.
- Note that this is not the same as parallel running.

* Users
:PROPERTIES:
:ID:       CEED7EB1-C9DD-40C6-ABBF-32D3E41FA6F7
:END:
- In the context of [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]], the "user" is either the actual person, or the program that they are running.

* UNIX
:PROPERTIES:
:ID:       C4CA2869-8F42-446C-A25A-570E4765A00C
:END:
- An operating system developed by AT&T
- Inspired GNU/Linux, which is not UNIX

* Motivations for [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]
:PROPERTIES:
:ID:       187C6FEC-1472-4AC3-9C78-CD345A297436
:END:

** Abstraction
:PROPERTIES:
:ID:       F81C54C3-E2DF-4E15-9679-0FA58A23E3B0
:END:
- Hide low-level details from the [[id:CEED7EB1-C9DD-40C6-ABBF-32D3E41FA6F7][user]]
- User can perform tasks without an understanding of what the OS is doing
- User can perform tasks without caring what the specifics of the hardware is
- Provides
  - Efficiency
  - Programmability
  - Portability

** Resource Allocator
:PROPERTIES:
:ID:       9A7A50F0-44D7-465F-A377-ADDF2D53A8FA
:END:
- Barrier between [[id:CEED7EB1-C9DD-40C6-ABBF-32D3E41FA6F7][users]] and the resources
  - CPU
  - Memory
  - I/O devices
  - Other hardware

** Control Program
:PROPERTIES:
:ID:       760712ED-0C18-4C5E-A5E7-B3B1770D6E92
:END:
- Prevent programs from misusing the computer
  - Both accidentally (due to bugs)
  - And purposely (viruses)
- Ensure isolation between the multiple [[id:CEED7EB1-C9DD-40C6-ABBF-32D3E41FA6F7][users]].
- Control execution of the programs
  - Security
  - Isolation
  - Protection
  - Prevent errors
  - Prevent improper use

* Modern [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]
:PROPERTIES:
:ID:       f770e083-fe6b-4e22-ae67-3292bda84695
:END:
- Examples:
  - Desktop
    - Windows
    - macOS
    - Linux
  - Mobile
    - iOS
    - Android
  - Embedded
    - Raspibian
  - RTOS
    - freeRTOS (ESP32 my beloved)
** Features of Modern (Desktop) OS
- Multitasking
  - Concurrent execution of programs on multiple cores
  - # of programs >>> # of cores, how?
  - Switch between programs very fast, just like people
- Multiuser
  - Multiple users can be logged in and use at the same time
- Variety of Hardware
  - Single PCs, shared memory systems (10-100s of processors), ...
** Features of Modern (Mobile) OS
- Customized verson of PC OS which has software dedicated to mobile haredware such as cellular modems
** Features of Embedded OS
- Operating system which needs to address specialized hardware
- Has to consider more restrictions such as power and hardware
- Not general purpose, only runs in specific environments
- Mostly stored in read only memory
** Features of Real-Time OS
- When applications need to deal with real time input-output data, RTOS is used
  - "Fly by wire", needs to respond instantly to inputs
- Cannot add new software without rewriting the code
- Can be soft (missable) or hard (cannot miss) time constraints
** Features of Distributed OS
- OS for large networks of computers which can be loosly or tightly connected

* OS Structure
:PROPERTIES:
:ID:       8212b8c8-23c8-445a-9cd4-2a9fc44950f9
:END:
- Implementation of [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]
- Factors:
  - Flexibility
    - Easy to adapt
  - Robustness
    - Hard to break
  - Maintainability
    - Easy fo sysadmins to change things
  - Performance
    - Low overhead
- Runs in the kernel [[id:79d9b1f3-2e86-41b0-a5a7-d56a31ada65d][Protection Mode]]
- Programs running under the operating system run in the user [[id:79d9b1f3-2e86-41b0-a5a7-d56a31ada65d][Protection Mode]]
- Libraries may directly interact with the hardware, others may talk to the OS instead
- System processes are OS processes that help with the functionality, but may be run under the user [[id:79d9b1f3-2e86-41b0-a5a7-d56a31ada65d][Protection Mode]].
- User programs may also talk to the OS directly, through the library, or directly to the hardware
- [[file:media/os-structure_1.png][OS Structure Diagram]]
* Kernel Organization
:PROPERTIES:
:ID:       dbdda23c-3747-4896-abec-6cd72a98cc93
:END:
- [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] also known as the kernel
  - Deals with hardware issues
  - Provides [[id:583386a9-eb89-491c-9de6-11cf052817da][System Call]] interface
  - Special code which allows user programs to use interrupt handlers and device drivers
  - Kernel code does not have access to system calls (of courses), normal libraries, or normal IO
    - "Code targets bare metal"
    - Code written in higher level compiled langauges like C/C++/ðŸ¦€
      - Previously written in assembly
    - Heaviliy hardware dependant
    - How do you debug
    - Code is split into machine independant HLL code, machine depandant HLL code, and assembly code
** Kernel Types
*** Monolithic Kernel
:PROPERTIES:
:ID:       89c5da4b-e47d-46be-b73a-db52dfc13241
:END:
- Large kernel which includes most of the non-user code running on the computer
- Drivers run within the monolith, which can cause BSOD/Crashes
- [[file:media/monolithic-kernel_1.png][Monolithic Kernel]]
*** Microkernel
:PROPERTIES:
:ID:       f932193f-1834-49e7-b2fa-631d4f008cec
:END:
- Small and clean
- Only essential services
- Other services are ran outside the kernel to provide resiliance
- [[file:media/microkernel_1.png][Microkernel]]

* Protection Mode
:PROPERTIES:
:ID:       79d9b1f3-2e86-41b0-a5a7-d56a31ada65d
:END:
- Hardware enforces protection modes which allow some instructions to only be run by certain privieged programs
- In a coarse overview, there are kernel and user modes
* VMs
:PROPERTIES:
:ID:       2240ad3a-9c58-44ab-adc8-f53388009f72
:END:
- What if you want to run more than one [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][Operating System]]?
- Use a virtual machine
- Virtualizes some underlying hardware that the OS expects
  - The layer in charge of this is called the [[id:a8b405f0-555f-43d2-af3f-93a38a43dd5c][Hypervisor]]
* Hypervisor
:PROPERTIES:
:ID:       a8b405f0-555f-43d2-af3f-93a38a43dd5c
:END:
- Type one hypervisor OS
  - Runs directly on the OS, may be stored in the ROM or BIOS
  - [[file:media/type-1-hypervisor_1.png][Type 1 Hypervisor]]
- Type two hypervisor
  - Runs above the OS
  - [[file:media/type-2-hypervisor_2.png][Type 2 Hypervisor]]
- WSL
- Docker/Kubernetes (container engines)

* Process Abstraction
:PROPERTIES:
:ID:       bbf11da3-2536-43e2-b1d7-93c46b3bf3ba
:END:
- Work queue
  - Workers take work from a work queue and execute them
- Master-slave
  - A main unit instructs worker units to work on different tasks
* Program Execution
:PROPERTIES:
:ID:       8245e915-ceba-4c96-9183-ce12f38f7b31
:END:
- Memory space is split amongst the different requirements of the program
- [[file:media/memory_1.png][Memory]]
- *Executable file format stores the instruction to the [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] on how to construct the process*
  - The file is a blueprint for the processes
  - Contains Instructions and Data
  - What address is the program located at?
  - During runtime, it also contains:
    - Text and Data (memory context)
** Hardware Layout
:PROPERTIES:
:ID:       2a0d3e9a-b060-4725-9cd2-882e52a9c356
:END:
- Memory is slower than registers
- Memory cache is used to speed up memory access
  - SRAM close (inside) the CPU
- Fetch unit grabs data from memory into the registers
  - Program Counter to indicate current program
- Registers come in general and special
  - General is used by programs to store data for instructions
  - Special is used for specific purpose
    - Stack Pointer
    - Program Counter
    - Frame Pointer
- [[file:media/hardware_1.png][Hardware]]
** Function Call
:PROPERTIES:
:ID:       8b9d71e1-fde5-4517-999a-64f413207b37
:END:
- Problems we need to solve:
  - How do we allocate data for the variables?
  - Where do we put the function such that it won't get mixed up?
- Caller function calls the Callee
  - Jump to the function body
  - Execute
  - Jump back to the original location
  - On the way, we need to store parameters and outputs
- These give rise to using a stack to store data
- In the stack, there will be a region called the [[id:1e0a4e58-4815-44c6-8872-000dd5c6e8b4][Stack Frame]]
- A stack pointer points to the top of the [[id:22f8b191-cc90-4c01-b9f5-10f78d597b42][Stack]]
- Basic Linkage
  - As calls are created, data is created on a stack
    - Command jump and link "jal" is used to jump to the callee and save the PC of the caller
    - Arguments stored in argument registers or the stack
  - As calls return, data is popped back off the stack
    - Jump back to the ra register
    - ra register must be saved if you want to further call a function
    - Save return values in return registers or the stack
    - Push back callee saved registers from the stack
- Frame-based linkage
  - The callee must:
    - Allocate space on the stack by creating a frame pointer with enough space to store all its variables, then move the stack pointer to where the frame pointer is
    - Stack may continue to grow if other functions are called
    - Finally, restore fp to the orginal value (since it is callee-saved) by adding to the stack pointer and storing in the frame pointer
  - This gives the function stack space to store variables
- The methods differ based on hardware and programming language, but must preserve certain states
  - This is known as the calling convention and must be consistant
** Stack
:PROPERTIES:
:ID:       22f8b191-cc90-4c01-b9f5-10f78d597b42
:END:
- A FIFO list
- Grows in one direction
- A stack pointer points to the top
- Composed by [[id:1e0a4e58-4815-44c6-8872-000dd5c6e8b4][Stack Frames]]
** Stack Frame
:PROPERTIES:
:ID:       1e0a4e58-4815-44c6-8872-000dd5c6e8b4
:END:
- Stores things like
  - Return address
  - Arguments
  - Local variable storage
  - Register data

* Least Significant Byte
:PROPERTIES:
:ID:       6a212478-657e-4121-a5f2-ad26de5cf35e
:ROAM_ALIASES: LSB
:END:
- The "smallest contributer" to a byte
- In 0xdeadbeef, it is 0xef

* Most Significant Byte
:PROPERTIES:
:ID:       0a64e439-8706-4401-ab6a-71577970d7aa
:ROAM_ALIASES: MSB
:END:
- The "largest contributer" to a byte
- IN 0xdeadbeef, it is 0xde

* Dynamically Alllocated Memory
:PROPERTIES:
:ID:       16e4886a-101b-4cf9-8e9d-f979234176b1
:END:
- When the scope of the data spans many procedure calls, you need dynamic allocation
- Lifetime can be as long as needed
- Languages:
  - C: malloc()
  - C++: new
  - Java: new
- Unlike local data: requires lifetime not tied to a process
- Unlike global data: needs to be allocated at runtime
- Solution: use heap
  - Heap is on the same side as the data and text of the program, and grows towards the stack (On Linux, heap grows towards higher addresses, downward in diagrams, stack grows towards lower addresses, upward in diagrams)
    - If they meet, too bad
  - Allocate heap space requires variable sizing, with variable allocation or deallocation timing
  - Create gaps during allocation
* Process
:PROPERTIES:
:ID:       f77c8ce6-2418-40ea-9cf2-0759fe185dfb
:END:
- A process is a dynamic abstraction for an executing program
- Includes
  - Memory Context
    - Code(Text), Data, Stack, Heap
    - All programs think they are running at memory location 0, but the OS has abstracted it out and virtualized another location in the memory
  - Hardware Context
    - Registers, Program counter, Stack pointer, Stack frame pointer
  - [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] Context
    - [[id:82259759-8435-4702-872d-c5ba1e790a2d][Process Management]] Properties
    - Resources used
* Process Management
:PROPERTIES:
:ID:       82259759-8435-4702-872d-c5ba1e790a2d
:END:
- The [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] must be able to switch between programs
- [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]] vs code
  - Code is just data on the hard disk
  - Processes are running code with allocated resources
** Process Identification
:PROPERTIES:
:ID:       1fa862c5-7ef2-4084-a884-b3f5e3f6f831
:ROAM_ALIASES: PID
:END:
- An unique ID number to distinguish two processes
- Issues:
  - Are they reused?
  - Does it limit the maximum number of processes?
  - Are there reserved PIDs?
** Process State
:PROPERTIES:
:ID:       3925466a-d2e6-4534-af63-0198ec7c2f90
:END:
- In multitasking, a [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]] can be:
  - Running
  - Not running
- The OS may run as a separate process or along with the current process
- A [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]] can also be Ready to run
- Each process has an associated process state associated with it
- [[file:media/5-state-process_1.png][5 State Process Model]]
  - Create
    - OS records information such as the memory context of the new process
    - "Creation process"
  - Admit
    - Process initialized
    - Ready to run
  - Scheduled
    - OS gives the CPU to the process
  - Release
    - The program or OS returns control back to the OS
  - Wait
    - The program signals that it needs an event to occur in order to continue
  - Event occurs
    - Pulls a process out of the blocked state into the ready state, meaning the event has occured and the program can continue execution
  - Exit
    - Program is done
  - New
    - Process just created
    - May still be initializing
  - Ready
    - Process is waiting to run on the CPU
  - Running
    - Process is running on the CPU
  - Blocked
    - The program is waiting for an event, it is not ready
    - Generally waiting for IO
    - Could also be waiting for a child process
  - Terminate
    - The OS must clear out the allocated resources to be used elsewhere
    - "Teardown process"
** Multicore management
:PROPERTIES:
:ID:       e3f37058-ba20-498b-b7d7-a6b17fd9a4eb
:END:
- With only 1 core, there is 1 running process and 1 transition at one time
- With m cores, there can be up to m running processes and multiple transitions
- Assumption in [[id:15F9BA90-8952-47DC-A1E9-951A8D12D158][CS2106]]: SINGLE CORE
** Process Queueing
:PROPERTIES:
:ID:       29fcb90f-8ae3-4196-b634-74412dc6b403
:END:
- [[file:media/process-queue_1.png][Process Queue]]
- Two queues: ready queue and blocked queue
** Process Creation
:PROPERTIES:
:ID:       9d9878a2-8dc4-44b5-93d8-f3d65c2f18b9
:ROAM_ALIASES: Forking
:END:
- AKA "Forking"
- To create a new [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]],
  - Clone yourself
  - Lay out the new process using the instructions from the executable
- In Linux, the fork and exec [[id:583386a9-eb89-491c-9de6-11cf052817da][Syscalls]] are used
- [[file:media/fork-c_1.png][Fork = Clone]]
  - The program state is exactly copied at the point in time after the fork call
  - Only difference is the return value of fork. The parent process will get the PID of the child, the child gets nothing
  - Use ~exec()~ to change to another executable
  - Use copy-on-write to prevent unnecessary cloning of information
- Parents should "reap" their child processes before exiting to cleanup
  - In C, the function is ~wait()~
  - NOTE: ~wait()~ does not block if there are no child processes to wait for ([[id:15F9BA90-8952-47DC-A1E9-951A8D12D158][CS2106 Exam]])
- A process will only have one parent unless,
  - Debugging: the child process could be a surrogate parent to another child (the debugger "attaches" to the process)
  - Parent killed: orphan processes will attach to another child process
  - Parent doesn't wait: child becomes a zombie process which isn't properly cleaned up
** Process Tree
:PROPERTIES:
:ID:       07f63771-0d15-43cb-b1d6-a9d59c2bd3bd
:END:
- ~pstree~ command in Linux
- Every process except a single parent process is a child of another process, giving rise to a tree
- The "mother of all processes" is that parent, in some Linux systems, it is Systemd.
** Process Control Block
:PROPERTIES:
:ID:       936f40d0-ff0e-4680-a31b-cd08d9ecf5e1
:ROAM_ALIASES: PCB "Process Table Entry"
:END:
- A block of data containing the entire execution context of a [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]].
- Contains hardware, memory, and [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] contexts.
** Process Table
:PROPERTIES:
:ID:       0a67a841-29cc-4614-a951-cf4e747e01f5
:ROAM_ALIASES: PCB
:END:
- The kernel stores [[id:0a67a841-29cc-4614-a951-cf4e747e01f5][PCBs]] for all processes
- Issues:
  - Scalability: how many processes can you run at the same time?
  - Efficiency: efficient access to PCBs vs space usage
- [[file:media/process-table_1.png][Process Table]]
* System Calls
:PROPERTIES:
:ID:       583386a9-eb89-491c-9de6-11cf052817da
:ROAM_ALIASES: Syscalls
:END:
- An API to interact with the [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]]
- Provides a way to call facilities or services existing in the kernel
- *NOT* the same as a normal function call
  - Requires [[id:79d9b1f3-2e86-41b0-a5a7-d56a31ada65d][Kernel Mode]]
- OS Dependent
  - UNIX: ~100 syscalls
  - Windows: ~1000 syscalls
- Languages
  - C/C++ can almost directly use syscalls
    - For example, ~getpid()~, or ~printf~ which calls ~write~ under the hood
    - These functions are provided by GNU LIBC
** Invoking System Calls
:PROPERTIES:
:ID:       b13d6413-611f-4c38-8930-7db3548bb23b
:END:
- User program invokes the library call
- Library call places the system call number in the correct register
- Library call executes a special instruction to enter [[id:79d9b1f3-2e86-41b0-a5a7-d56a31ada65d][Kernel Mode]]
  - Instruction is usually called TRAP
- In the kernel mode, the system call handler is determined based on the value in the register
  - Uses the value as an index for searching for the appropriate handler
  - Handled by a "dispatcher"
- System call is executed
- System call handler ends
  - Return control to the calling library
  - Switch back to user mode
- Library call returns to the user program
- [[file:media/syscall-process_1.png][Syscall Process]]
* Handler Routines
:PROPERTIES:
:ID:       50712286-ce75-4c85-b77c-218514d2ab5e
:END:
- After an interruption:
- [[file:media/interrupt_1.png][Interruption]]
- The hardware will transfer control to the handler routine
- Program execution may resume
** Exceptions
:PROPERTIES:
:ID:       c2323338-f9ba-4ba0-bf10-1c0d94151b37
:END:
- Sometimes machine level instructions can cause errors
- Examples:
  - Divide by 0
  - Illegal address access
- The hardware will force a [[id:583386a9-eb89-491c-9de6-11cf052817da][Syscall]] execute the exception handler
- Execution is synchronous, i.e. right after the error
** Event Handler
:PROPERTIES:
:ID:       3e26cb31-e015-4a89-ba3e-4cdbcedefafa
:END:
- External events may interrupt a program
- Usually hardware inputs
- Interrupts are asynchronous, meaning that it happens outside of program execution flow
- The interrupt handler will be called
* Concurrent Execution
:PROPERTIES:
:ID:       5f016241-05b6-4c76-b61f-a06964ed1e88
:END:
- Concurrent processes are processes that multiple processes can progress in execution at the same time
- Could be virtual parallelism: illusion of process running at the same time
  - AKA [[id:adfa430b-a10a-4e21-8025-cd95f5a8ecb2][Timeslicing]]
- Could be physical parallelism: processes are actually running at the same time on multiple CPUs
  - AKA [[id:adfa430b-a10a-4e21-8025-cd95f5a8ecb2][Timeslicing]] on multiple processors

* Timeslicing
:PROPERTIES:
:ID:       adfa430b-a10a-4e21-8025-cd95f5a8ecb2
:END:
- [[file:media/interleaved-execution_1.png][Interleaved Execution]]
- Sharing a CPU's time amongst many processes
- Every time you switch processes, the [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] needs to decide which [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]] to switch to
  - Involves an overhead with context switching
- Problems
  - What happens when you have more processes to run than you do CPUs? Then how do you choose which to run?
    - This is known as the scheduling problem
    - Scheduler: the part of the OS which makes the decision
    - [[id:330baf6a-5e30-4f41-8847-f9eddf19f71e][Scheduling Algorithm]]: the algorithm used by the scheduler
** Scheduling Algorithm
:PROPERTIES:
:ID:       330baf6a-5e30-4f41-8847-f9eddf19f71e
:END:
 - Need to consider the amount of CPU time required by each process
 - Divide the processes into two classes:
   - CPU-Activity, most time is spent doing calculations
   - IO-Activity, most time is spent waiting for IO
 - Type of process:
   - Batch, low priority because no user interaction is required, no responsivity requirements
   - Interactive, should be responsive to user
   - Real time processing, has a deadline for something to be completed
 - The algorithm may be influenced by the current processing enviroment, but it has to balance between conflicting criteria:
   - Fairness, each process should get enough CPU time, no process should be starved
     - Applies per user or per process
   - Utilization, all parts of the computing system should be constantly in use
 - Two types of scheduling policies:
   - Non-preemptive (cooperative)
     - Process will stay scheduled until it blocks or yields to other processes
   - Preemptive
     - Process will be evicted based on its time quota, it will be removed whether or not it blocks once the timer is up.
     - The OS will come in even when there is only one process
*** Scheduling for Batch Processing Systems
- Criteria:
  - Turnaround time (finish time - arrival time)
  - Throughput (how many tasks per time)
  - Makespan (total time for all tasks, only applicable when the set of tasks is fixed)
  - CPU utilization (percent of time when CPU is working on tasks)
- Generally easier to implement and understand
  - First come first serve
    - Tasks stored in a Queue data structure based on arrival time
    - Run the first task in the queue to block, yield, or completion
    - Place the task to the back of the queue if it is not completed
    - Guarantees no starvation as the number of tasks left to run before a process gets the CPU is always decreasing
    - Averge waiting time can be further optimized by reorganizing
      - Convoy Effect
      - If I/O-bound tasks are queued after CPU-bound tasks, then the I/O bound tasks are idling when they could be waiting on I/O if given a short amount of time to run
  - Shortest job first
    - Select tasks with smallest CPU time
    - May not be known, sometimes musst be guessed at
      - Common approach is exponential average
        - \(P_{n+1}=wA_n +(1-w)P_n\)
        - Where P = prediction, A = actual, and w is how we weight the actual vs the prediction
    - Good for minimizing average wait time
    - May starve long jobs if shorter jobs are consistently being added
      - In fact, being able to not starve the longest job requires the CPU to be exacty at or less than full utilization compared to the volume of tasks coming in
  - Shortest remaining time
    - Variation of shortest job first with preemtion
    - If a new job is added with a shorter remaining time, the current process may be evicted to run it.
    - Allows short jobs arriving later to not have to wait for longer jobs
*** Scheduling for Interactive Systems
- Criteria:
  - Response Time (time between requests and response)
  - Predictability (low variance in wait time)
- Utilizes preemtive scheduling by creating timer based interrupt to hand control over to the OS
- Algorithms:
  - Round Robin
    - All tasks are stored in a FIFO queue
    - When each task blocks, yields, or completes, we evict it from the CPU
    - The task is moved to the back of the queue if it is not completed
    - Guarantees each tasks can get some CPU time every \(q(n-1)\) units, where q is the [[id:1fd3da26-d080-477b-b2bd-da4a0c914a09][Time Quantum]] and n is the number of tasks
    - [[id:b3ef1808-f8f2-4f6b-89a3-c9dfb5046182][ITI]] and [[id:1fd3da26-d080-477b-b2bd-da4a0c914a09][Time Quantum]] must be carefully chosen
  - Priority Scheduling
    - Some tasks are more important than others
    - Assign priority to each process
      - (Always clarify whether low number or high number have higher priority)
    - Select tasks with higher priority to run first
    - May starve low priority tasks
    - Possible solutions:
      - Decrease the priority every time quantum
      - Don't consider the process for the next round of scheduling
    - Hard to guarantee the amount of CPU time given to a process using priority
    - [[id:a0007235-a2a1-4d4f-99f7-99fbe63ededc][Priority Inversion]]
  - Multilevel Feedback Queue
    - How do we schedule without perfect knowledge?
    - Learn along the way
    - Minimize both Response time for IO bound process, and turnaround time for CPU bound process
    - Basic Rules:
      - If Priority(A)>Priority(B), then A should run
      - If Priority(A)=Priority(B), then they run in round robin
    - Priority setting/changing
      - New jobs given maximum priority
      - If a job fully uses the [[id:1fd3da26-d080-477b-b2bd-da4a0c914a09][Time Quantum]], then its priority is reduced
      - Otherwise, no change
      - In some systems, when tasks are dropped to the lowest priority then some are re-elevated
  - Lottery Scheduling
    - Give lottery ticket for each process for each resource
    - For each scheduling decision, randomly choose one ticket to grant that process
    - In the long run, a process with X% of the tickets for a resource can use the resource X% of the time
    - Lottery tickets can be distributed to children
    - Important process given more tickets
    - Each resource has its own set of tickets
** Scheduling Process
:PROPERTIES:
:ID:       6809f87c-1a74-4932-9d98-141bd54e191b
:END:
- When schedule is triggered:
  1. [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] decides if context switching is necessary
  2. The OS picks a suitable process to run next, based on the [[id:330baf6a-5e30-4f41-8847-f9eddf19f71e][Scheduling Algorithm]]
  3. The OS sets up the context for the [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Process]]
  4. Process is allowed to run
* Interval of Timer Interrupt
:PROPERTIES:
:ID:       b3ef1808-f8f2-4f6b-89a3-c9dfb5046182
:ROAM_ALIASES: ITI
:END:
- How long between when the [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] is handed control.
- Generally 1-10ms.
- Decreasing it increases overhead
* Time Quantum
:PROPERTIES:
:ID:       1fd3da26-d080-477b-b2bd-da4a0c914a09
:END:
- How long each task is given to run on the CPU.
- Generally 5-100ms.
- Decreasing it increases overhead but increases responsiveness
* Priority Inversion
:PROPERTIES:
:ID:       a0007235-a2a1-4d4f-99f7-99fbe63ededc
:END:
- Occurs when a resource is locked by a lower priority process, which is then supersceded and unable to unlock the resource
- Then, high priority tasks which needs that resource cannot run and lower priority tasks will be run instead
* Isolation vs Cooperation
:PROPERTIES:
:ID:       805bb2b2-1af9-46fe-9af1-04b2c13e0f4d
:END:
- Under the OS, processes run as if it is the only process in the system
- Sometimes processes must cooperate and send data
  - Share nothing (client server model)
  - Shared memory (may be prone to abuse or accidents)
- Solution: use [[id:5eb73086-a3c9-43f2-98a6-c864225b6ef3][IPC]]
* Inter-Process Communication
:PROPERTIES:
:ID:       5eb73086-a3c9-43f2-98a6-c864225b6ef3
:ROAM_ALIASES: IPC
:END:
- Mechanisms introduced in Unix System V
  - Shared memory
  - Message passing
  - Semaphores
** Shared Memory
:PROPERTIES:
:ID:       25e99b54-148b-41c1-9a5e-105bf59c80fd
:END:
- A process creates a shared memory region
- That shared memory region is appended to another process' memory
  - The virtual address for the memory region is not guaranteed to be the same across processes
- Processes can now write and read to the memory in order to communicate
- OS only needs to manage the creation and sending of shared memory region to other processes, rest is done by processes
- Pros:
  - Efficient (OS has very little to do)
  - Easy to Use (behaves like normal memory)
- Cons:
  - Synchronization: requires locks and ordering
  - Implementation is harder
** Message Passing
:PROPERTIES:
:ID:       c6ff3ba9-c369-40d7-8123-d2418df8930b
:END:
- Simply give a function which allows processes to send messages to other processes
- Message first stored in OS memory space
- Processes must know some way to identify the other
  - Direct communication: sockets
    - Sender and receiver knows the socket / name of the other
    - One link per pair of processes
    - Need to know the ID of the other party
  - Indirect communication: mailbox/port
    - Processes deposit and read messagefs from mailboxes
    - Mailbox can be shared
- Can block (synchronous) to wait for a message, or not (asynchronous) and just check if the message is there to receive.
- Pros:
  - Synchronization: each process knows that a message has actually been sent, whereas in shared memory each process may be unsure if the message has been sent
  - Portable: Conceptually can be extended to different environments, such as over a network
- Cons:
  - Inefficient: needs OS intervention and copying of messages
* Pipes
:PROPERTIES:
:ID:       36f3628e-a51f-40b6-afef-bb38a8ca55f5
:END:
- A mainly UNIX concept
- Links the output of one process to the input of another
- In the command line: ~A | B~ sends the STDOUT of A to STDIN of B
  - Process A is the producer, which writes bytes to a virtual buffer
  - Process B is the consumer, which reads bytes from that buffer
  - The buffer behaves like a FIFO queue of bytes, like an anonymous file
- Pipe behaves as a "circular bounded byte buffere with implicit synchronization"
  - Fixed max size
  - Writers will wait when the buffer is full
  - Readers will wait when the buffer is empty
- Has multi-producer and multi-consumer variants, but generally we only consider the simple case
- Can be full-duplex where both parties can read and write
- Pipes are made with the ~pipe()~ syscall
- File descriptors are duplicated with the ~dup()~ and ~dup2()~ syscalls
* Signals
:PROPERTIES:
:ID:       4c09e377-8418-4c2f-aba8-bf48b0506d9e
:END:
- Signals are asynchronous notifications sent to the process
- Some signals have default handlers, but programs can provide their own handlers
* Threads
:PROPERTIES:
:ID:       1ad82130-71b0-4ec2-b8af-1fb02fae19bf
:END:
- Motivations
  - Processes are expensive, so [[id:9d9878a2-8dc4-44b5-93d8-f3d65c2f18b9][Forking]] is inefficient
    - ~fork()~ requires duplication of memory space and program context
  - Context switching also takes resources
  - IPC is hard with independant memory spaces, which requires [[id:5eb73086-a3c9-43f2-98a6-c864225b6ef3][IPC]]
- Threads are built to make parallel execution easier
- A traditional process only has a single thread, so only a single instruction is executing at any time
- Instead, we add more threads to execute multiple parts of the program
- Threads share:
  - Memory context
  - OS Context
- Threads have unique:
  - ID
  - Registers
    - Includes PC
  - "Stack"
    - Under the hood, it just modifies the \(fp and \)sp headers
- Threads are more lightweight
- Questions:
  - What happens when system calls are made in parallel?
  - What happens when you ~fork()~ a multithreaded process?
  - What happens when a single thread exits?
  - What happens when a single thread calls ~exec()~?
- Hardware support exists within modern CPUs
  - Intel Hyperthreading
** User Threads
:PROPERTIES:
:ID:       7ea2c32e-f16b-4cb1-92aa-368576cff979
:END:
    - Thread is a user library
    - Kernel is unaware of threads
    - [[file:media/user-threads_1.png][Diagram]]
    - Advantages:
      - Works on any OS
      - Simply library calls
      - More flexible
    - Disadvantages:
      - OS cannot schedule at the thread level
** Kernel Threads
:PROPERTIES:
:ID:       31a4c5f8-16b8-4cc4-a783-473ebd9a67d1
:END:
- Kernel now schedules threads instead of processes
- [[file:media/kernel-threads_1.png][Diagram]]
- Advantages:
  - Kernel can schedule at the thread level
- Disadvantages:
  - Less flexible
  - Thread operations require system calls (more expensive)
** Hybrid Thread Model
:PROPERTIES:
:ID:       d4445289-6dfc-40b0-8e49-b71865f6ab36
:END:
- Combines ideas from [[id:7ea2c32e-f16b-4cb1-92aa-368576cff979][User Threads]] and [[id:31a4c5f8-16b8-4cc4-a783-473ebd9a67d1][Kernel Threads]]
- OS schedules on kernel threads
- User threads can be bound to kernel threads
- Provides more flexibility
- [[file:media/hybrid-threads_1.png][Diagram]]
** POSIX Threads
:PROPERTIES:
:ID:       f6db6e39-c578-4e9b-8069-37b142f0c4c8
:END:
- AKA pthreads
- Defines the API of thread behavior
- Can be implmented either as user or kernel threads
- Defined using a routine/function, its arguments, and some extra attributes with ~pthread_create(mut ref ThreadID, ThreadAttr, Routine, RoutineArgs)~
- Thread will exit on return, or on a call to ~pthread_exit()~
- Thread can be joined or waited on with a call to ~pthread_join()~
  - By default, we do not know when the threads will be executed, so we usually use joining to synchronize
- pthreads have a lot more options such as:
  - Yielding
  - Advanced synchronization
  - Scheduling policies
  - Binding to kernel threads
* Synchronization
:PROPERTIES:
:ID:       239f9594-8127-40b4-b7fd-2acc158cfb2c
:END:
- Concurrent execution may lead to non-deterministic outcomes
  - This means that the outcome depends on the order in which processes / instructions are run
** Race Condition
:PROPERTIES:
:ID:       9c2cc20c-45b4-41d9-a1aa-72faa17f33d1
:END:
- When access to mutable resources occurs in an unsynchronized manner
- [[file:media/race-condition_1.png][Race Condition]]
- Generally solved by marking out certain sections of code as "critical", and preventing other threads from running critical sections while a certain thread is
- Critical Section Must:
  - Only be run by at most one process at a time (mutual exclusion)
  - Allow a waiting process to enter it once it is no longer in use (progress)
  - Limit the maximum number of times other processes can enter the section before any particular waiting process (bounded wait)
  - Processes not in the critical section should not block other processes (independance)
- Critical Section Must Not Have:
  - Deadlock, where all processes are blocked (happens when processes have cyclic dependancies and wait for each other)
  - Livelock, where processes do useless work like changing state to avoid deadlock, but makes no meaningful process
  - Starvation, where a particular waiting process never gets to enter the critical section
** Atomic Instruction
:PROPERTIES:
:ID:       4e685ce2-19fc-44d4-a7ce-433014ba8829
:END:
- Instructions that can run within a single machine operation, which provides certain guarantees about everything being successful or nothing being done at all
** Test and Set
:PROPERTIES:
:ID:       76b57868-3982-487c-abd0-b3ad6822861a
:END:
- An [[id:4e685ce2-19fc-44d4-a7ce-433014ba8829][Atomic Instruction]] which loads data from memory and sets the data in memory to 1
- This can be use to implement a lock
- [[file:media/simple-testandset_1.png][Example]]
  - This is a good start and works, but results in busy waiting (the program is in the ready state while running the while loop)
** Peterson's Algorithm
:PROPERTIES:
:ID:       9add2553-fdd5-4ebd-89f1-4e75567f0358
:END:
- Use two mechanisms: a turn counter and a wait array
- In the case of two processes, we have a turn counter (0 or 1), and wait[0] and wait[1]
- Want indicates which process wants to enter the CS
- Turn indicates which process should be given priority
- [[file:media/petersons-algorithm_1.png][Diagram]]
- Assumes that writes to Turn are atomic, otherwise it could be overwritten while the other process checks the value
- Problems:
  - While loop is a busy loop, and uses CPU time instead of going into a blocked state
    - Busy waiting should be employed for short waits to avoid blocking overhead
    - Blocked state should be employed for longer waits when blocking overhead is less than the cost of busy waiting
  - Low level, which requires interacting with many variables
  - Not general, doesn't work for general synchronization
** Semaphore
:PROPERTIES:
:ID:       927e6d69-9220-448c-a23c-a74746e763e6
:END:
- High level concept which provides general synchronization
- Only specifies desired behaviors, but ignores implementation
- Invented by Dijkstra
- Provides
  - A way to block a number of processes
  - A way to wake up one or more sleeping processes
- Stores an integer
- Defines Two Operations (Atomic!)
  - Wait
    - While the value of S is \(S \leq 0\), block/sleep
    - Decrement S (upon wake up if blocked, otherwise as normal)
    - AKA P() or Down()
  - Signal
    - Increment S
    - Wake up one sleeping process, if any
    - This operation never blocks
    - AKA V() or Up()
- [[file:media/semaphore_1.png][Diagram]]
- Properties:
  - Given initial value of S is \(S_{\text{initial}} \geq 0\)
  - \(S_{\text{current}}=S_{\text{initial}}+numSignal(S)-numWait(S)\)
    - where numSignal is number of signal operations completed and
    - numWait is the number of wait operations completed
    - Requires each process to call signal after every wait
- Types
  - General semaphore, takes any integer \(S \geq 0\)
  - Binary semaphore, takes values 0 or 1
    - It has the same power as a general semaphore, that is, a general semaphore can be implemented given a semaphore.
- Solves all known synchronization problems
** Mutex
:PROPERTIES:
:ID:       3e85ccda-9e07-4e37-9f1f-3b14451abd34
:END:
- [[file:media/mutex_1.png][Diagram]]
- A usage of a [[id:927e6d69-9220-448c-a23c-a74746e763e6][Semaphore]] which allows the programmer to restrict a critical section to only one process at a time.
- [[file:media/mutex-proof_1.png][Proof]]
- [[file:media/mutex-proof_2.png][Proof 2]]
- Usage of Semaphores incorrectly may still lead to deadlock:
  - [[file:media/mutex-deadlock_1.png][Deadlock]]
  - In general, when we have a cycle in the wait graph, then deadlocks may occur
** Classical Synchronization Problems
:PROPERTIES:
:ID:       49bbc7b1-f852-4e7f-97e4-c505a274d1d0
:END:
*** Producer Consumer
:PROPERTIES:
:ID:       7938563c-4d8d-4eea-bd35-58648ab02b79
:END:
- Processes share a bounded buffer with a bounded size
- Producers insert items into buffer (waits when buffer is full)
- Consumers remove items from buffer (waits when buffer is empty)
- Busy waiting version:
  - [[file:media/producer-consumer-busy-wait_1.png][Busy Wait]]
- Blocking version:
  - [[file:media/producer-consumer-blocking_1.png][Blocking]]
*** Readers Writer
:PROPERTIES:
:ID:       b74cf2d0-fda7-4410-a0b8-8e5a6cf4d089
:END:
- Multiple readers read from data simultaneously
- Only a single writer can write to the data
- Data cannot be read to and wrote to simultaneously
- [[file:media/reader-writer_1.png][Example]]
  - In this case, the writer may starve if there is a continuous stream of readers
*** Dining Philosophers
:PROPERTIES:
:ID:       c804675c-5a9b-4073-88f3-0082bd20110a
:END:
- We have N philosophers sitting around a table, with single chopsticks in between each one
- Each philosopher can only eat by aquiring the chopsticks to their left and their right
- We want a deadlock and starve free for all philosophers to eat
- Attempt 1:
  - [[file:media/philosophers-deadlock_1.png][Deadlock]]
  - This will deadlock when all philosophers pick up their left chopsticks simultaneously
  - What if we put the left chopstick down if we cannot get the right?
  - Leads to a livelock where the philosophers will repeatedly pick up and put down the left chopstick
- Attempt 2:
  - [[file:media/philosophers-slow_1.png][Slow]]
  - Works, but does not allow simultaneous eating
- Tannenbaum Solution
  - [[file:media/philosophers-tannenbaum_1.png][1]]
  - [[file:media/philosophers-tannenbaum_2.png][2]]
    - Note the macro use for LEFT and RIGHT make them automatically relative to i (disgusting, I know)
  - [[file:media/philosophers-tannenbaum_3.png][3]]
- Limited Eater Solution
  - Leave an empty seat
  - By PHP there is at least one person who can eat
* Memory Management
:PROPERTIES:
:ID:       69beabef-f618-4755-91aa-18f4dae1359c
:END:
- [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] wants to
  - Allocate memory for new [[id:f77c8ce6-2418-40ea-9cf2-0759fe185dfb][Processes]]
  - Manage memory space for processes
  - Protect memory space of processes from each other
  - Provide memory related system calls to processes
  - Manage memory space for internal use
** Physical Address
:PROPERTIES:
:ID:       b7583c52-f40d-4f08-a274-5a3b764ffd49
:END:
- Addresses referring to bytes in physical [[id:b7aa1fcd-4bc1-4cb2-aa78-121f452bb11a][Memory Hardware]]
** Logical Address
:PROPERTIES:
:ID:       ada5e17f-72ee-4e61-8eef-435db9805cba
:END:
- How a process views the memory space
- Logical address is generally not equal to the physical address
- Each process is granted its own logical memory space
  - That way, the process cannot even name the logical address that would correspond to another process's [[id:b7583c52-f40d-4f08-a274-5a3b764ffd49][Physical Address]]
- After compiling, multiple output programs can be combined into one by a linker, which will rearrange the memory such that the resultant program still starts at 0
- Before running, the loader will randomize the address space before loading the program
** Memory Hardware
:PROPERTIES:
:ID:       b7aa1fcd-4bc1-4cb2-aa78-121f452bb11a
:END:
- RAM
- Essentially an array of bytes
- Each byte has an index, known as the [[id:b7583c52-f40d-4f08-a274-5a3b764ffd49][Physical Address]]
- It is a "contiguous memory region", meaning that it refers to an interval of consecutive addresses, each address can be used.
** Memory Hierarchy
:PROPERTIES:
:ID:       8922a543-7084-4b43-b081-afd9cb1a461d
:END:
- From fastest to slowest, smallest to largest, most expensive to cheapest:
  1. Registers
  2. Cache
  3. [[id:b7aa1fcd-4bc1-4cb2-aa78-121f452bb11a][Memory Hardware]]
  4. Hard Disks
  5. Offline Storage
** Memory Usage
:PROPERTIES:
:ID:       73e4f238-986d-43f5-a8a1-35cd16587e18
:END:
- Memory allocated by the [[id:D289CD47-38F4-481F-BED1-FEAF25C4D709][OS]] per process
- Two types of data:
  - Transient data, valid for limited duration like a function call, such as local vars
  - Persistent data, valid for the entire duration like a global variable
- Possible for data to outlive process using files
** Memory Abstraction
:PROPERTIES:
:ID:       e885be59-61c0-40d5-9249-74105ac5462c
:END:
- Without memory abstraction:
  - The address in the program corresponds directly to the [[id:b7583c52-f40d-4f08-a274-5a3b764ffd49][Physical Address]]
    - Simple, but this causes conflicts between conflicts that cannot be resolved since these issues must be addressed at compile time
    - Makes it hard to protect your memory space
  - No conversion or mapping required
  - Address is fixed during compile time
- First attempt: Address Relocation
  - All programs still assume starting at location 0
  - When loading the process into memory, update the memory references by some offset determined by the OS
    - Causes slow loading time
    - Must have some mechanism to distinguish between regular values and memory addresses
- Second attempt: Base and limit registers
  - Set a base register corresponding to the base address for memory access
  - Set a limit register corresponding to the first address not allocated to that process
  - [[file:media/base-limit_1.png][Diagram]]
  - Memory access a bit slower:
    - Must set Actual = Base + Addr, check that Actual < Limit
- Use [[id:ada5e17f-72ee-4e61-8eef-435db9805cba][Logical Address]]
** Contiguous Memory Management
:PROPERTIES:
:ID:       faaa6bff-952d-45fd-b367-1f314cf72a08
:END:
- Assumptions:
  - Each process is granted a contiguous memory region
  - Our [[id:b7aa1fcd-4bc1-4cb2-aa78-121f452bb11a][Memory Hardware]] has space to contain the complete memory space of multiple processes.
- To support multitasking:
  - Multiple processes in memory at once
- When the memory is full
  - Move blocked processes to secondary storage (hard disk)
- Two schemes of allocating partitions
  - Fixed size
    - Physical memory space is split up into a fixed number of partitions
    - A process runs on one of the partitions
    - [[file:media/fixed-partition_1.png][Diagram]]
    - Easy to manage, fast to allocate
    - Partition sizing must be able to accomodate the largest process
    - Wastes space for processes smaller than the partition size
      - Known as "internal fragmentation", because the empty space is within the space allocated to a process
  - Variable size
    - Partition is created based on process size
    - OS dynamically allocates and frees partitions
    - Free memory space known as holes, with a lot of process creation we get a lot of smaller holes
      - Known as "external fragmentation", because the empty space is outside the space allocated to the process
      - OS will do compaction to combine the smaller holes into larger, more useful holes
        - Generally time consuming and not run frequently
    - More flexible and removes internal fragmementation
    - More informaiton needs to be recorded by OS, takes more time to locate the appropriate memory region
    - Allocation:
      - OS maintains a list of partitions and holes, generally stored using linked list
        - [[file:media/dynamic-allocation_1.png][Diagram 1]]
        - [[file:media/dynamic-allocation_2.png][Diagram 2]]
      - For a process with size N, algorithm wants to locate space of size M such that M > N
      - Possible strategies:
        - First-Fit: pick the first hole with M > N
        - Best-Fit: pick the smallest hole with M > N
        - Worst-Fit: pick the largest hole
      - After allocating, the hole is split into partition of size N and a hole of size M - N
    - On free:
      - Merge the freed partition with adjacent holes
  - Variable size: buddy system
    - Allow some amount of internal fragmentation
    - When allocating memory for a process, a free block is split into half repeatedly to meet request
    - The two halves form "buddy blocks"
    - When a pair of "buddy blocks" are free, they can recombine into a larger block.
    - [[file:media/buddy-block_1.png][Diagram]]
    - Implementation
      - [[file:media/buddy-system_1.png][Diagram 1]]
      - [[file:media/buddy-system_2.png][Diagram 2]]
      - [[file:media/buddy-system_3.png][Diagram 3]]
      - [[file:media/buddy-system_4.png][Diagram 4]]
      - [[file:media/buddy-system_5.png][Diagram 5]]
      - [[file:media/buddy-system_6.png][Diagram 6]]

* Disjoint [[id:69beabef-f618-4755-91aa-18f4dae1359c][Memory Management]]
:PROPERTIES:
:ID:       8c551088-5881-427a-b121-6178c8ee713f
:ROAM_ALIASES: Page
:END:
- Needed due to various constraints
- We remove the assumption that applications need contiguous memory regions
  - (Still keep the assumption that physical memory is large enough for the whole process)
- Allocate memory space in disjoint [[id:b7583c52-f40d-4f08-a274-5a3b764ffd49][Physical Address]] locations using a mechanism known as [[id:f9e7c37c-5658-4381-8238-1a4dd3be9a7f][Paging]]
** Paging
:PROPERTIES:
:ID:       f9e7c37c-5658-4381-8238-1a4dd3be9a7f
:END:
- [[id:b7583c52-f40d-4f08-a274-5a3b764ffd49][Physical]] memory is split into regions of fixed size, known as physical frames
- [[id:ada5e17f-72ee-4e61-8eef-435db9805cba][Logical]] memory is split into regions of the same size, known as logical pages
  - Different size is technically possible but leads to many complications with address translation
- Each logical page is loaded into any available memory frame, resulting in a continuous logical space mapped onto a potentially disjoint physical memory space
  - Loaded on a need-to basis, upon access
- Under such a scheme, we need to keep track of the map between each logical page and its associated physical frame (known as a page table)
  - [[file:media/disjoint-memory_1.png][Diagram]]
- Properties
  - This results in no external fragmentation, since all free frames can be used equally
  - This still results in internal fragmentation, since the required memory space may not be exactly a multiple of page size
    - Small page size reduces this problem, but lowers efficiency
  - Achieves a clear separation of logical and physical address space which provides flexibility with simple address translation
  - Basic paging scheme can be extended to provide additional features by appending data to the page table
    - Access-Right bits
    - Validity bit
      - Also allows for shrinking page tables
    - Each table entry can use these control mechanisms to check that memory access is authorized and valid
  - Page sharing can allow multiple processes to share physical memory frames
    - Shared code/libraries can take advantage of this by having code that can be utilized by multiple processes
    - Copy-on-Write lowers the memory overhead forked children such that they do not have to clone the full memory space and the data inside it, only the page table
      - The frame will only be copied when a process writes to it
      - [[file:media/copy-on-write-page_1.png][Diagram]]
      - OS uses flags to detmerine if this case occurs
- Problems
  - Pure software solution requires two accesses to memory for each read from memory
    - One to read the page lookup table, one to access the actual item
    - Solution: use a [[id:2f9b6ef7-13ee-4307-b9e1-eed014212e21][Translation Look-Aside Buffer]]
*** Page Address Translation
:PROPERTIES:
:ID:       b412830a-a54a-4117-b76a-80957d547510
:END:
- The process by which we translate a [[id:ada5e17f-72ee-4e61-8eef-435db9805cba][Logical Address]] to a [[id:b7583c52-f40d-4f08-a274-5a3b764ffd49][Physical Address]]
- Find which page the logical address is in, and the offset
- Find the frame the page is mapped to
- Add the offfset to the frame's base address (frame number times frame size) to get the physical address
  - This will be sped up if we keep the frame size as powers of 2, which will allow for easy multiplication and division (standard 4K, 4096)
  - This will be easier if the frame size is equal to the page size
    - Starting address of a frame and starting address of a page must always have n trailing 0 bits
  - [[file:media/address-translation_1.png][Diagram]]
  - [[file:media/address-translation-formula_1.png][Formula]]
*** Translation Look-Aside Buffer
:PROPERTIES:
:ID:       2f9b6ef7-13ee-4307-b9e1-eed014212e21
:ROAM_ALIASES: TLB
:END:
- Implement a special cache in hardware to store a few page table entries, known as the Translation Look-Aside Buffer
- [[file:media/page-cache_1.png][Diagram]]
- [[file:media/page-cache-efficiency_1.png][Efficiency]]
- TLB becomes part of the hardware context of a process
- Should be flushed upon context switch
- Inefficient to purposefully refill the cache upon returning, so just leave it empty and let the cache fill itself
** Segmentation
:PROPERTIES:
:ID:       ca257976-6526-475a-b6d0-8c61d026244d
:END:
- So far, we treat memory as a single entity
- However, in typical programs, we have different regions: program code/text, data, heap, stack, library code, etc.
- These regions have different properties/uses: some may need to grow or shrink, such as the heap
  - If placed in a contiguous memory space, it is hard to grow or shrink.
- Solution: place each type of data into its own segment
  - In x86 systems, each even has its own register, not really used these days
- Each memory segment has a name
- Logical memory space is now a collection of segments
- Memory access is now specified by using segment name and offset
- Segment sizes can vary, unlike a [[id:8c551088-5881-427a-b121-6178c8ee713f][Page]]
- Properties
  - Allows segments to grow or shrink independently
  - Can be protected and shared independently
  - May cause external fragmentation
*** Segment Address Translation
:PROPERTIES:
:ID:       3c122185-d935-42d8-8d8c-043a3cd5600e
:END:
- Logical addresses defined using Segment ID and Offset
- Each segment name is represented as a single ID by the compiler
- The segment ID is used to look up information from the segment table (Base address, limit, permissions, etc.)
- Physical address calculated as base address + offset
- [[file:media/segmentation_1.png][Diagram]]
- [[file:media/segmentation_2.png][Diagram 2]]
  - This may be implemented in hardware
** Segmentation with Paging
:PROPERTIES:
:ID:       a5f3d3a0-fe66-4ac7-b26a-3cfe8a537788
:END:
- Combine both [[id:ca257976-6526-475a-b6d0-8c61d026244d][Segmentation]] and [[id:f9e7c37c-5658-4381-8238-1a4dd3be9a7f][Paging]]
- Each segment has its own page table, allowing it to grow and shrink in a non-contiguous fashion
  - Alternatively, some systems only keep one page table and all segment pages refer to that single table
- [[file:media/segmentation-paging_1.png][Diagram]]
- [[file:media/segmentation-paging_2.png][Diagram 2]]

* Virtual Memory
:PROPERTIES:
:ID:       3dc19324-195a-4d9d-aff1-2a7a54258689
:END:
- Now we can store memory in non-contiguous regions, but we still have the assumption that our physical memory can satisfy all the demand
- What happens if this assumption fails? How can we operate when our logical memory is larger than physical memory?
- Two different logical memory locations may point to the same physical memory location
- Solution: use our secondary storage, which is much larger and cheaper, to back up our physical memory
  - Some of our pages will be stored within the secondary storage
  - When the page is needed, the data will be loaded into physical memory
- We need some mechanism to tell whether or not a page is currently "memory resident", i.e. whether it is loaded in the physical memory
- When a process wants to access a non-memory resident page, a page fault will be triggered
- The OS must handle the page fault to load the page into memory
  - [[file:media/virtual-memory_1.png][Process]]
  - [[file:media/virtual-memory_2.png][Diagram]]
- Another reason why we need sufficiently large page sizes: reduce the number of page faults, which have significant overhead
** Thrashing
:PROPERTIES:
:ID:       82e329e3-fafd-40a9-9f24-951ba60a9f66
:END:
- Secondary access time >> physical memory access time
  - If page faults occur during most memory accesses, then many non-memory resident pages need to be loaded
  - Known as thrashing
- "How likely is a page to be used once being loaded?"
- Locality principles:
  - Most programs will access memory within a small region of code and data very frequently
  - Temporal locality: memory addresses accessed will be likely to be accessed again
    - Amortize the loading cost across many access instances
  - Spacial locality: memory addresses close to one accessed are likely to be referenced
    - Access to nearby addresses will not cause page fault
- Of course, some programs can be specifically designed or poorly designed to do such a thing
** Page Table Structure
:PROPERTIES:
:ID:       63559cfd-e93b-405a-bf24-241fa083411d
:END:
- How to structure page table for efficiency?
- Large memory space leads to larger page tables
- Large page table leads to:
  - Higher access overhead
  - Fragmentation (page table may span multiple pages)
- Direct Paging:
  - With \(2^p\) pages in logical memory space, our page table needs:
    - \(p\) bits per entry to specify a unique page
    - \(2^p\) entries, each containing the corresponding physical frame and additional information bits
- Two-level Paging:
  - Process may not utilize the full physical memory space, so a full page table is a waste
  - Split the direct page table into smaller page tables
  - Each has a page table "directory number"
  - Each smaller page table has size equivalent to one page
  - If we have \(2^M\) smaller page tables, then we need \(M\) bits to identify a unique page table
  - Then, each smaller page table has \(2^{P-M}\) entries, and needs \(P-M\) bits to identify a unique entry
  - In total, we still have \(P\) bits to uniquely define one entry within the 2-level paging
  - Some page table numbers do not have assigned smaller page table, which saves memory
  - Only memory in regions that are needed will have assigned page tables
    - Accessing memory outside these regions lead to segfaults
  - Therefore, we need one page directory and n page tables, where n is the number of regions that we need to allocate.
  - [[file:media/2-level-paging_1.png][Diagram]]
- In Intel CPUS, we actually use 4-level page tables:
  - Same idea as going from one to two levels
  - Note: addresses in the entries are PHYSICAL (duh)
  - Each table has size 4KiB (1 page)
  - Each entry is 64 bits long
- Inverted page table
  - Normally, page table stored per process
  - It is difficult to find which frames are used and by which process
  - Wasted entries: out of M tables, only N entries are valid
  - Instead, we store a single mapping between a frame number to a <pid, page#> tuple (note that the direction is inverted, i.e. we go from physical to virtual instead of virtual to physical)
  - Advantages:
    - save space, only one table for all processes
    - easier frame management
  - Disadvantages:
    - Slow lookup of addresses
  - [[file:media/inverted_1.png][Diagram]]
** Page Replacement Algorithms
:PROPERTIES:
:ID:       67038e75-fb34-4916-b801-2aa1561ceb9d
:END:
- How to decide which pages get kicked out of memory?
- When we encounter a page fault when no frames are available, one must be evicted to make space for a new one to be loaded
- Before eviction:
  - If it is clean (no edits), no need to write back
  - If it is dirth (edited), then we need to copy back to storage
- Memory access time:
  - \(T_{access} = (1-p)T_{mem} + pT_{page fault}\)
  - Where \(p\) is the probability of page fault
  - Since the page fault timing is much larger than the memory access time, a good algorithm should minimize the chance of page fault
- Algorithms:
  - Optimal
    - Always chooses the best option
    - Replace the page that will not be used for the longest period of time
    - Guarantees the lowest number of page faults
    - Impossible in practice, since it requires extra information about the future
    - Used to compare with other algorithms
  - FIFO
    - Replace the oldest page
    - OS maintain a queue of resident pages, and the front of the queue is evicted when replacement is required
    - Simple to implement with no hardware support needed
    - Problems:
      - As the number of frames increases, there are situations where FIFO performs worse (more page faults)
      - Such a situation is known as Belady's Anomaly
      - This is due to FIFO not exploiting temporal locality
  - Least Recently Used
    - Replace the page used least recently
    - Exploit temporal locality by assuming recently used pages will be used soon after
    - Emulates optimal solution by assuming that pages not recently accessed will probably not be accessed for a long time
    - Does not suffer from Belady's Anomaly
    - However, implementation is hard: we need to keep track of "last access time" which is impossible without hardware support
      - Especially hard to order the page accesses
      - Method 1: use a counter
        - Counter increments for each access
        - Save the count to the page table entry
        - We need to search through a lot of pages to locate the least recently used
        - May overflow!
      - Method 2: "stack"/deque/list
        - Maintain a stack of page numbers
        - If page X is used, remove from stack and place on top
        - Replace the page on the bottom of the stack
        - Faster to locate the least recently used
        - However, the "stack" is not very efficient to remove from the middle, especially hard to implement within hardware
  - Second chance page replacement (CLOCK)
    - Modified FIFO
    - Each page table entry now includes an "Accessed" referenced bit
    - The oldest FIFO page is selected
      - If the reference bit is 0, replace that page
      - If the reference bit is 1, set it to 0 and move it to the back of the queue (load time is set to be newly loaded), then try the next FIFO page
    - Degenerates to FIFO when all reference bits are the same (all 0 or all 1)
    - Implemented using a circular queue
** Frame Allocation Policies
:PROPERTIES:
:ID:       258abbd0-2ba7-4c13-b72b-cafce2b621b4
:END:
- How to distribute physical memory frames among the processes?
- If we have \(N\) frames and \(M\) processes
- Equal Allocation:
  - Each process gets \(N/M\) frames
- Proportional Allocation:
  - Each process has a size \(\text{size}_p\), totaled to \(\text{size}_{tot}\) among all processes
  - Each process is allocated \(N \text{size}_p / \text{size}_{tot}\)
- Local replacement:
  - Victim pages chosen amongst pages of the process that causes the page fault
  - Frames remain constant, leading to stable performance
  - If a process doesn't have enough allocated frames, then performance is hindered
- Global replacement:
  - Victim can be from any process
  - Dynamic adjustment between processes
  - Processes can hog frames
  - Frames can differ between runs
- If we have insufficient physical frames:
  - Heavy I/O with disk is needed
  - If global replacement, [[id:82e329e3-fafd-40a9-9f24-951ba60a9f66][Thrashing]] processes will steal frames from other processes, causing cascading Thrashing
  - If local replacment, Thrashing is localized, but that process will use up disk bandwidth
- Working Set Model
  - In a certain "locality", a process will have a certain set of pages that in needs
  - Few page faults will be triggered once those pages are loaded, until the process moves to an new locality
  - Define a working set window \(\delta\)
  - Define \(W(t, \delta)\) where \(W(t, \delta)\) is the number of active pages in the interval from time \(t - \delta\) to \(t\)
  - Try to allocate enough frames for \(W(t, \delta)\)
