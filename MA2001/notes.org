:PROPERTIES:
:ID:       7039eb35-01ee-43ef-8334-74824a443069
:END:
#+title: MA2001 Notes
#+filetags: :MA2001:

- links :: [[id:c05cbeec-6a23-4ac8-ab3d-a0fbd3dfbe40][MA2001]]

* Linear Equation
:PROPERTIES:
:ID:       3bee8ee1-8e7c-476e-9b88-fa42b9a5c67d
:ROAM_ALIASES: Line
:END:
- A line in the XY plane is something that can be represented by the equation \(ax + by = c\), such that at least one of \(a\) and \(b\) are not 0.
- A line in \(n\) variables has the form \(a_1 x_1 + a_2 x_2 + ... + a_n x_n = b\) where \(a_1\), \(a_2\), ..., \(a_n\), and b are real constants.
* Zero Linear Equation
:PROPERTIES:
:ID:       8ca5b581-af4b-44c2-99a6-fe3c0a725edf
:END:
- A [[id:3bee8ee1-8e7c-476e-9b88-fa42b9a5c67d][Linear Equation]] where all the constants are zero.
* Non-Zero Linear Equation
:PROPERTIES:
:ID:       13d3821f-2ef7-4cc7-a344-d74ffb302286
:END:
- A [[id:3bee8ee1-8e7c-476e-9b88-fa42b9a5c67d][Linear Equation]] which is not a [[id:8ca5b581-af4b-44c2-99a6-fe3c0a725edf][Zero Linear Equation]].
* Solution of a Linear Equation
:PROPERTIES:
:ID:       0ac0637e-128d-4efa-8791-b673199d42f9
:END:
- A soltion to a [[id:3bee8ee1-8e7c-476e-9b88-fa42b9a5c67d][Linear Equation]] is a set of values that when assigned to the variables, the equation holds true.
* Solution Set
:PROPERTIES:
:ID:       6d6c3bfc-2b01-4012-b255-d155b17fdc04
:END:
- The set of all [[id:0ac0637e-128d-4efa-8791-b673199d42f9][Solution of a Linear Equation]].
- Examples:
  - For a [[id:8ca5b581-af4b-44c2-99a6-fe3c0a725edf][Zero Linear Equation]], all ordered tuples are valid solutions.
  - For a [[id:3bee8ee1-8e7c-476e-9b88-fa42b9a5c67d][Linear Equation]] where all constants are zero except the non-coefficient, the solution set is the empty set.
* General Solution
:PROPERTIES:
:ID:       f2b18f06-e5f6-49b6-adc2-d89b51db1fa9
:END:
- An expression which produces the [[id:6d6c3bfc-2b01-4012-b255-d155b17fdc04][Solution Set]].
- Generally given by writing all variables in terms of some number of arbitrary parameters.
  - In general, the arbitrary parameters is related to the [[id:5ba9a192-afee-4100-9f47-f6a4f81b57de][Dimension]] of the equation.
- Examples:
  - \(4x - 2y = 1\) has the general solution \(\begin{cases}x=t\\y=2t-\frac{1}{2}\end{cases}\) (not unique)
* System of Linear Equations
:PROPERTIES:
:ID:       4de0b32f-feb4-42e9-8bca-194ed2408758
:ROAM_ALIASES: "Linear System"
:END:
- A finite set of [[id:3bee8ee1-8e7c-476e-9b88-fa42b9a5c67d][Linear Equation]]
- When lined up with variables in the same column, the coefficients take the form \(a_{\text{Row number},\text{Column number}}\)
- Examples:
  - \(\begin{cases}a_{11} x_1 + a_{12} x_2 = b_1\\a_{21} x_1 + a_{22} x_2 = b_2 \end{cases}\)
* Solution of a Linear System
:PROPERTIES:
:ID:       a03cd843-9760-47c4-9069-ca75d9aec0e4
:END:
- The set of values that when assigned to the variables, all equations in the [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][System of Linear Equations]] hold true.
- A linear system can have
  - No solutions ([[id:dd4a724b-7288-448f-8be9-7e885699af51][Inconsistant Linear System]])
  - Exactly 1 solution ([[id:b2f4051d-ee59-40dc-a561-4ea4f14fb79f][Consistant Linear System]])
  - Infinitely many solutions ([[id:b2f4051d-ee59-40dc-a561-4ea4f14fb79f][Consistant Linear System]])

* Inconsistant Linear System
:PROPERTIES:
:ID:       dd4a724b-7288-448f-8be9-7e885699af51
:END:
- A [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][System of Linear Equations]] without any solutions
* Consistant Linear System
:PROPERTIES:
:ID:       b2f4051d-ee59-40dc-a561-4ea4f14fb79f
:END:
- A [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][System of Linear Equations]] with at least one solution

* Augmented Matrices
:PROPERTIES:
:ID:       ecb5ed8b-d483-4934-ab99-f3c4858e2e31
:END:
- A [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] which represents a [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][System of Linear Equations]] with the coefficients transcribed such that each row is one equation, and the last column is the non-coefficient constant.
  - Conventionally, there is a dividing line between the last column and the rest of the matrix.
- Examples:
  - \(\begin{cases}1 x_1 + 2 x_2 = 3\\4 x_1 + 5 x_2 = 6 \end{cases}\) is the same as \(\left[\begin{array}{cc|c}1&2&3\\4&5&6\end{array}\right]\)
* Elementary Row Operations
:PROPERTIES:
:ID:       bf26f901-fa17-4c98-b8bf-8d54d5a8f01d
:END:
- Operations you can do on an [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]].
  - Multiply the row by a nonzero constant
    - Notation for multiplying the ith row by \(c\) is \(cR_i\)
  - Interchange two rows
    - Notaion for changing the ith and jth row is \(R_i \leftrightarrow R_j\)
  - Adding a multiple of a row to another row
    - Notation for adding c times of the jth row to the ith row is \(R_i + cR_j\)
    - Can be 0, since adding an equation multiplied by 0 does not change anything
- Multiple operations are not considered elementary.
- These operations mirror the algebraic operations of adding and multiplying equations.
- After an arbitrary number of these operations, the [[id:a03cd843-9760-47c4-9069-ca75d9aec0e4][Solutions]] remain the same. (See [[id:03fd9b35-48e8-42da-a4bf-bb6942228e03][Row Equivalent]])
* Row Equivalent
:PROPERTIES:
:ID:       03fd9b35-48e8-42da-a4bf-bb6942228e03
:END:
- Two [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrices]] are considered row equivalent iff they can be tranformed into one another by applying a series of [[id:bf26f901-fa17-4c98-b8bf-8d54d5a8f01d][Elementary Row Operations]].
- Two row equivalent matrices have the same [[id:a03cd843-9760-47c4-9069-ca75d9aec0e4][Solution]].

* Leading Entry
:PROPERTIES:
:ID:       6800c4a5-7da7-4464-90a7-d587d9e37beb
:END:
- The leftmost non-zero number in a row of an [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]]

* Pivot Point
:PROPERTIES:
:ID:       a9286172-954d-4085-b8a6-af330ee19d0d
:END:
- The [[id:6800c4a5-7da7-4464-90a7-d587d9e37beb][Leading Entry]] in a row of an [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] in [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][Row-Echelon Form]]

* Pivot Column
:PROPERTIES:
:ID:       2e30e6b2-9251-4c46-996d-f73cc7a14329
:END:
- A column of an [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] which contains a [[id:a9286172-954d-4085-b8a6-af330ee19d0d][Pivot Point]].
- If the last column is a pivot column, then the system is a [[id:dd4a724b-7288-448f-8be9-7e885699af51][Inconsistant Linear System]]
- If all columns except the last column is a pivot column, then the system has one solution

* Row-Echelon Form
:PROPERTIES:
:ID:       b0ddc803-3424-4cd1-b60e-4311c41a6098
:ROAM_ALIASES: REF
:END:
- An [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] with the properties:
  - All rows entirely composed of zero (which represent [[id:8ca5b581-af4b-44c2-99a6-fe3c0a725edf][Zero Linear Equations]]) are at the bottom of the matrix.
  - The [[id:6800c4a5-7da7-4464-90a7-d587d9e37beb][Leading Entry]] in a row must be *strictly* to the right of the leftmost non-zero number of the row above.
- If a matrix R is a [[id:03fd9b35-48e8-42da-a4bf-bb6942228e03][Row Equivalent]] matrix of matrix A, then R is called a REF of A, and A is said to have a REF of R (*not unique!*)
* Reduced Row-Echelon Form
:PROPERTIES:
:ID:       4546b235-26c0-4582-b238-594d24f1360e
:ROAM_ALIASES: RREF
:END:
- An [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] which is in [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][Row-Echelon Form]] and also has the properties:
  - The [[id:6800c4a5-7da7-4464-90a7-d587d9e37beb][Leading Entry]] is always 1
  - In each [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Column]], all values (other than the [[id:a9286172-954d-4085-b8a6-af330ee19d0d][Pivot Point]]) are zero.
- While in RREF, the solutions can be read directly from the matrix.
- All matrices have a unique RREF
* Back Substitution
:PROPERTIES:
:ID:       1fd4e4db-bdb7-4352-8cfa-699cd2ebcf6e
:END:
- Method for solving while in [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]]
- While in REF, the solutions can be found as so:
  - Set all non-pivot columns as arbitrary parameters
  - Solve for the pivots in terms of constants and the arbitrary parameters
    - This can be done by going from the last non-zero row and working your way up.
  - If the last non-zero row's [[id:6800c4a5-7da7-4464-90a7-d587d9e37beb][Leading Entry]] is the rightmost number, then the system has no solutions

* Gaussian Elimination
:PROPERTIES:
:ID:       fb8122ac-2940-4312-be6b-e26198af619d
:END:
- An algorithm to reduce a [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] to a [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]] matrix.
  - Locate the leftmost nonzero column
  - Interchange the top row with another row to make the entry of that column in the top row non-zero
  - For all rows below the top, add a multiple of the new top row to it to make the value in the chosen column 0
  - Continue by ignoring the top row, then repeating the previous steps with the rest of the rows
  - The matrix will eventually reach [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]]
* Gaussian-Jordan Elimination
:PROPERTIES:
:ID:       d75d83b9-0a43-4b94-b94e-1321627fa2b8
:END:
- An algorithm to reduce a [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] to a [[id:4546b235-26c0-4582-b238-594d24f1360e][RREF]] matrix.
  - First apply [[id:fb8122ac-2940-4312-be6b-e26198af619d][Gaussian Elimination]]
  - Multiply each row by a constant such that the leading entry becomes 1
  - Starting from the last [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Column]], add multiples of the row of that [[id:a9286172-954d-4085-b8a6-af330ee19d0d][Pivot Point]] to the rows above in order to set the values in that column to 0.
* Solve for Constants to Ensure Solutions
:PROPERTIES:
:ID:       3cbffdb7-6c89-469a-af22-f760a906f0c0
:END:
- Instead, solve for when there are no solutions
  - Express the [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrix]] in a form such that you can determine where the [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Columns]] are by conducting [[id:fb8122ac-2940-4312-be6b-e26198af619d][Gaussian Elimination]]
  - Use the properties of [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Column]] to determine when there are no solutions
  - May need to do casework on the unknowns if you need to divide (cannot divide by 0)
    - To avoid somewhat, don't use a unknown at the top as the pivot
    - In general, you cannot divide or multiply by an unknown which could be 0
    - You are, however, allowed to add a row multiplied by an unknown which could be 0 since if it is 0, nothing changes
* Geometrical Interpretation
:PROPERTIES:
:ID:       de666d50-86a6-4390-8606-bb621d089416
:END:
- In a [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]], each non-zero row corresponds to a [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Column]].
  - Therefore, the number of non-zero rows must be less than or equal to the number of variables plus 1, which is the maximum number of [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Columns]].
  - In a [[id:b2f4051d-ee59-40dc-a561-4ea4f14fb79f][Consistant Linear System]], the number of non-zero rows must be less than or equal to the number of variables, since the last column must not be a [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Column]]
- The number of arbitrary parameters is given by the number of variables minus the number of pivot columns.
* Homogenous Linear System
:PROPERTIES:
:ID:       98073aba-eb13-435c-b020-535c499f2767
:END:
- A homogenous linear system is a [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][Linear System]] where all the non-coefficient constants are equal to zero.
- All homogenous systems have the solution \(x_1 = 0\), \(x_2 = 0", ..., "\)x_n = 0". This is called the trivial solution
- All other solutions are known as non-trivial solutions
  - Therefore homogenous linear systems have either one solution or infinitely many solutions
  - If there are more variables than rows then there are infinitely many solutions
* Matrix
:PROPERTIES:
:ID:       d7a14523-8aca-462b-96d7-96f8c198e3e9
:END:
- A m by n matrix has m rows and n columns
- The (i, j) th entry of the matrix is in the ith row and the jth column
* Column Matrix
:PROPERTIES:
:ID:       caffc8a6-388b-4a79-86e0-8c1301215c15
:END:
- A [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] with only one column
* Row Matrix
:PROPERTIES:
:ID:       6c4371a6-1c7a-4823-b4fe-2a2b98ebcd76
:END:
- A [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] with only one row
* Square Matrix
:PROPERTIES:
:ID:       390d4ad5-a285-441e-a435-b613c1ad4190
:END:
- A [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] with the same number of rows and columns
* Diagonal of a Square Matrix
:PROPERTIES:
:ID:       25ed9f16-6ad2-4bf5-a8d1-ae1ac9f32608
:END:
- The sequence of entries \(a_{11}, a_{22}, ..., a_{33}\) in a [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]]
* Diagonal Matrix
:PROPERTIES:
:ID:       60269c0a-1614-4f10-bb35-6831c702d103
:END:
- A [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] called a diagonal matrix if all values not in its [[id:25ed9f16-6ad2-4bf5-a8d1-ae1ac9f32608][Diagonal]] are zero
* Scalar Matrix
:PROPERTIES:
:ID:       3e5d983a-458a-4a8e-b1df-4862f4d1ce25
:END:
- A [[id:60269c0a-1614-4f10-bb35-6831c702d103][Diagonal Matrix]] where all values in its [[id:25ed9f16-6ad2-4bf5-a8d1-ae1ac9f32608][Diagonal]] are equal
* Identity Matrix
:PROPERTIES:
:ID:       ec6a800b-bfc4-4498-9158-1a209cf0e2c5
:END:
- A [[id:3e5d983a-458a-4a8e-b1df-4862f4d1ce25][Scalar Matrix]] where all the values in its [[id:25ed9f16-6ad2-4bf5-a8d1-ae1ac9f32608][Diagonal]] are equal to 1
- Can be notated by \(\mathbb{I}_n\) for a n by n identity matrix
* Zero Matrix
:PROPERTIES:
:ID:       d65d98af-766e-4be4-8c44-7b054a7848a7
:END:
- A [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] where all the values are 0.
- Can be notated by \(0_{m\times n}\) for a m by n identity matrix
  - The subscript can be dropped if it is clear what the size is
* Symmetrical Matrix
:PROPERTIES:
:ID:       a518a757-1129-4a2b-bc67-da392434ad7e
:END:
- A [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] where \(a_{i,j} = a_{j, i}\) for all \(i\) and \(j\)
* Upper Triangular Matrix
:PROPERTIES:
:ID:       42792e44-1879-489f-b58e-32f01fec557f
:END:
- A [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] where \(a_{i,j} = 0\) for all \(i > j\)
* Lower Triangular Matrix
:PROPERTIES:
:ID:       c26f10dd-0798-45eb-be5f-837e5aad51bd
:END:
- A [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] where \(a_{i,j} = 0\) for all \(j > i\)
* [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] Operations
:PROPERTIES:
:ID:       279de6eb-6af8-41fd-bc58-71687659bbce
:END:
** Matrix Equality
:PROPERTIES:
:ID:       444fe283-2f38-45c4-a1bc-a75a83f22c8a
:END:
- Two [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] are equal when corresponding entries are all equal (size must be equal too)
** Matrix Addition
:PROPERTIES:
:ID:       0152a8ec-6aa4-4510-9562-a5026200f17e
:END:
- Two [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] are added by adding corresponding entries (size must be equal)
** Matrix Subtraction
:PROPERTIES:
:ID:       280f46d1-a2bf-4474-9f82-5a0fb260bff3
:END:
- Two [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] are subtracted by subtracting the entry in the second matrix from the one in the first
** Matrix Scalar Multiplication
:PROPERTIES:
:ID:       4c80d776-74bb-42bb-a07b-b2b88b8deeaf
:END:
- A [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]] is multiplied by a scalar by multiplying each entry by that scalar
** Basic Properties of Matrix Operations
:PROPERTIES:
:ID:       55248702-1ef5-4612-b117-45d4936392c8
:END:
- Let A, B, C be equally sized [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] and c and d be scalars
- Commutative property of addition: \(A + B = B + A\)
- Associative property of addition: \(A + B + C = (A + B) + C = A + (B + C)\)
- Scalar multiplication distributes into addition: \(c(A + B) = cA + cB\)
- Also: \((c + d)A = cA + dA\)
- Also: \((cd)A = c(dA) = d(cA)\)
- Zero addition: \(A + 0 = 0 + A = A\) (0 here means the appropriately sized [[id:d65d98af-766e-4be4-8c44-7b054a7848a7][Zero Matrix]])
- \(A - A\) = 0
- \(0A = 0\)
** Matrix Multiplication
:PROPERTIES:
:ID:       76f39bdc-085f-459e-a0e3-533fe66188d4
:END:
- To multiply two [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] \(A \times B\):
- Number of columns in A must be equal to the number of rows in B.
- If A is \(m \times p\) and B is \(p \times n\), then \(A \times B\) is \(m \times n\)
- The (i, j)th entry is equal to \(\sum_{k=1}^{p} a_{ik} b_{kj}\)
- Not commutative
- "pre-multiplication" of A to B = \(AB\)
- "post-multiplication" of A to B = \(BA\)
** Properties of [[id:76f39bdc-085f-459e-a0e3-533fe66188d4][Matrix Multiplication]]
:PROPERTIES:
:ID:       e79dec60-803d-4c5b-9987-c90efe15cbec
:END:
- If A/A_n, B/B_n, C/C_n are dimentioned such that the multipliation is defined and c is a scalar, then
- \(A(BC) = (AB)C\)
- \(A(B_1 + B_2) = AB_1 + AB_2\)
- \((C_1 + C_2)A = C_1A + C_2A\)
- \(c(AB) = (cA)B = A(cB)\)
** Power of a [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]]
:PROPERTIES:
:ID:       77806fef-4ae7-484e-ae51-6e62b3de6ae4
:END:
- Let A be a [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]]
- \(A^n\) is the [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]] when n is 0
- \(A^n\) is A times itself n times total when n is not 0
- Because multiplication is not commutative, we cannot say \((AB)^2\) = \(A^2B^2\)
** [[id:76f39bdc-085f-459e-a0e3-533fe66188d4][Matrix Multiplication]] Notation
:PROPERTIES:
:ID:       88db205b-1cfb-40b3-ad13-5827454de810
:END:
- \(AB = A[b_1 b_2 b_3 ... b_n] = [Ab_1 Ab_2 Ab_3 ... Ab_n]\), essentially writing B as a list of columns
- Similarly, \(AB = \begin{bmatrix}a_1\\a_2\\a_3\\...\\a_n\end{bmatrix} = \begin{bmatrix}Ba_1\\Ba_2\\Ba_3\\...\\Ba_n\end{bmatrix}\)
** Transpose of a [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]]
:PROPERTIES:
:ID:       c4062e3e-4992-4d7f-8621-984120945600
:END:
- Each column becomes a row, flipping the dimention
- The (i, j)th entry of the matrix is the (j, i)th entry of the transpose of the matrix
- Denoted as \(A^T\)
** Properties of [[id:c4062e3e-4992-4d7f-8621-984120945600][Transpose of a Matrix]]
:PROPERTIES:
:ID:       500dce9e-2036-4fe7-8652-c3b4a94e6576
:END:
- \((A^T)^T=A\)
- \((A + B)^T = A^T + B^T\)
- \((cA)^T=cA^T\)
- \((AB)^T=B^T A^T\)
* Proof of Matrix Equality
:PROPERTIES:
:ID:       041eb1cb-cb5e-4efb-aa43-cf6755e64235
:END:
- Both [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] must have same size
- Must have matching entries
- Show both properties somehow to get the proof
* Representing Solutions of [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][Linear Systems]] as a [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]]
:PROPERTIES:
:ID:       2d595692-0b5d-4f84-95b7-e4c2da4b070f
:END:
- Simply write the solutions as a column matrix
- Then, the product of the [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrices]] not including the non-coefficient constant (last column), to the solutions matrix, is equal to the column removed (non-coefficient constants)
* Inverses of [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrices]]
:PROPERTIES:
:ID:       1fa53496-0920-4486-8a8f-3cbc26d118ed
:END:
- If we have \(AX = B\), in matrix-world, how do you find \(X\)?
- We want to find an "inverse" of \(A\) such that \(A A^{-1} = I\) and \(A^{-1} A = I\) (\(I\) is the [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]])
- A matrix \(A\) is invertiable iff there exist a matrix \(B\) such that \(AB=BA=I\))
- In this case, \(B\) and \(A\) are inverses of each other
- A square matrix with no inverse is known as singular
** Cancellation Laws
:PROPERTIES:
:ID:       94d4982d-bd1c-4d6e-a4cd-1aec31b2729e
:END:
- Let A be an invertible matrix
- If \(AB_1 = AB_2\) then \(B_1 = B_2\)
- If \(B_1A = B_2A\) then \(B_1 = B_2\)
** Uniqueness of Inverses
:PROPERTIES:
:ID:       d7988dc7-802b-49bd-aa49-a71d161886ba
:END:
- If \(B\) and \(C\) are inverses of \(A\) then \(B=C\)
- The unique inverse of \(A\) is denoted by \(A^{-1}\)
** Inverse of a 2x2 Matrix
:PROPERTIES:
:ID:       0b76b425-f331-4cc4-9d6a-d7ff33e40bb7
:END:
- Let \(A = \begin{bmatrix}a&b\\c&d\end{bmatrix}\)
- Then \(A^{-1} = \frac{1}{ad-bc}\begin{bmatrix}d&-b\\-c&a\end{bmatrix}=\begin{bmatrix}\frac{d}{ad-bc}&\frac{-b}{ad-bc}\\ \frac{-c}{ad-bc}&\frac{a}{ad-bc}\end{bmatrix}\)
** Powers of an Inverse Matrix
:PROPERTIES:
:ID:       ce6fa8ea-505a-4a06-82ea-005afb2b91df
:END:
- \(A^{-n}\) is \(A^{-1}\) times itself \(n\) times
** Properties of Invertible Matrices
:PROPERTIES:
:ID:       a4fb6efc-f6c2-4484-a35f-27527827b839
:END:
- If A and B are invertible and c is not 0, then
- \(cA\) is invertible, \((cA)^{-1}=\frac{1}{c}A^{-1}\)
- \(A^T\) is invertible and \((A^T)^{-1}=(A^{-1})^T\) ([[id:c4062e3e-4992-4d7f-8621-984120945600][Transpose of a Matrix]])
- \(A^{-1}\) is invertible and \((A^{-1})^{-1}=A\)
- \(AB\) is invertible and \((AB)^{-1}=B^{-1}A^{-1}\)
  - More generally, \((A_1 A_2 A_3...A_n)^{-1} = A_n^{-1} A_{n-1}^{-1} A_{n-2}^{-1}...A_1^{-1}\)
- \(A^r A^s = A^{r+s}\) ([[id:ce6fa8ea-505a-4a06-82ea-005afb2b91df][Powers of an Inverse Matrix]])
- \(A^n\) is invertible and \((A^n)^{-1}=(A^{-1})^n\)
** Properties Equivalent to Invertability
:PROPERTIES:
:ID:       5c656b0b-ab52-4d87-89b7-268c6aa6661c
:END:
- The following are all equivalent:
  - \(A\) is [[id:1fa53496-0920-4486-8a8f-3cbc26d118ed][Invertible]]
  - The linear system \(Ax=0\) has only the [[id:98073aba-eb13-435c-b020-535c499f2767][Trivial Solution]]
  - The [[id:4546b235-26c0-4582-b238-594d24f1360e][RREF]] form of \(A\) is an identity matrix.
  - \(A\) can be expresed by the product of [[id:dee66fdd-85a5-403e-b7e6-65b062c19307][Elementary Matrices]]
  - \(det(A) \neq 0\)
  - The rows of \(A\) form a [[id:b3caafea-8006-4974-b41a-13c2917d32c9][Basis]] for \(\mathbb{R}^n\)
  - The columns of \(A\) form a [[id:b3caafea-8006-4974-b41a-13c2917d32c9][Basis]] for \(\mathbb{R}^n\)
  - \(A\) is full-rank
  - 0 is not an [[id:9098094c-3584-417f-bece-3218f546e590][Eigenvalue]] of \(A\)

* Elementary Matrices
:PROPERTIES:
:ID:       dee66fdd-85a5-403e-b7e6-65b062c19307
:END:
- Terms defined under [[id:ecb5ed8b-d483-4934-ab99-f3c4858e2e31][Augmented Matrices]] are all valid under normal [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]] as well
- An elementary matrix is a [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] which can be obtained by performing an [[id:bf26f901-fa17-4c98-b8bf-8d54d5a8f01d][Elementary Row Operation]] to an [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]].
* Applying [[id:bf26f901-fa17-4c98-b8bf-8d54d5a8f01d][Elementary Row Operations]] to [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrices]]
- The operations are applied to an [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]] to obtain the appropriate [[id:dee66fdd-85a5-403e-b7e6-65b062c19307][Elementary Matrix]] E, then E is multiplied to the matrix you want to operate on (E \times A), in that order
- The effect is the same
- To multiply the ith row by k, take the [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]] and multiply the (i,i)th entry by k
  - For example, to multiply the second row by 2:
    - \(\begin{bmatrix}1&0\\0&2\end{bmatrix}\)
  - Note: if \(k \neq 0\) then the operation has an [[id:1fa53496-0920-4486-8a8f-3cbc26d118ed][Inverse]] by swapping \(k\) with \(\frac{1}{k}\)
- To swap the ith row with the jth row, take the [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]] and swap the ith row and the jth row
  - This is equivalent to setting the (i,i)th entry and the (j,j)th entries to 0, and setting the (i,j)th and (j,i)th entries to 1
  - For example, to swap the first and second rows:
    - \(\begin{bmatrix}0&1\\1&0\end{bmatrix}\)
  - Note: this matrix is its own [[id:1fa53496-0920-4486-8a8f-3cbc26d118ed][Inverse]]
- To add a multiple of the ith row to the jth row, take the [[id:ec6a800b-bfc4-4498-9158-1a209cf0e2c5][Identity Matrix]] and add k to the (j,i)th entry
  - For example, to add two times of the first row to the second row:
    - \(\begin{bmatrix}1&0\\2&1\end{bmatrix}\)
  - Note: to invert, swap \(k\) with \(-k\)

* Counting Forms of a [[id:4546b235-26c0-4582-b238-594d24f1360e][RREF]]
:PROPERTIES:
:ID:       05bfdb22-3129-4e5c-ba9a-3b0913ceb454
:END:
- links :: [[id:064e0f67-745e-4d9f-b4f1-a178e273d2aa][MA2001 Exam]]

- Two forms are different if they have a different set of pivot points.
- Forms may have unknown values which are usually denoted by *, this still only contributes one to the count.
- Need to consider cases with zero rows

* Finding Inverses
:PROPERTIES:
:ID:       770cccac-1c27-432a-ab09-0510d927fd92
:END:
- Using [[id:5c656b0b-ab52-4d87-89b7-268c6aa6661c][Properties Equivalent to Invertability]]
- Find [[id:dee66fdd-85a5-403e-b7e6-65b062c19307][Elementary Matrices]] \(E_1...E_k\) such that \(E_k E_{k-1}...E_1 A=I\)
- \(E_k E_{k-1}...E_1 = A^{-1}\)
- To find these matrices, write the \(n \times 2n\) matrix \((A | I)\)
- We know that \(E_k E_{k-1}...E_1(A|I)=(I|A^{-1})\), so to find the matrices we do [[id:d75d83b9-0a43-4b94-b94e-1321627fa2b8][Gaussian-Jordan Elimination]] on \((A|I)\)
  - If we don't get a matrix of the form \((I|A^{-1})\) as the result, there is no inverse.

* Singular Matrices
:PROPERTIES:
:ID:       19f4d185-2389-455b-afb7-7d341beb5453
:END:
** Properties of Singular Matrices
:PROPERTIES:
:ID:       7ad24bb1-4688-47e2-a3be-1c4551859e15
:END:
- If \(A\) is singular, then \(AB\) and \(BA\) are singular

* Elementary Column Operations
:PROPERTIES:
:ID:       962804fa-c96b-49f0-8da1-94fe6ba3e26d
:END:
- Instead of applying [[id:bf26f901-fa17-4c98-b8bf-8d54d5a8f01d][Elementary Row Operations]] by multiplying the [[id:dee66fdd-85a5-403e-b7e6-65b062c19307][Elementary Matrix]] on the left, you multiply on the right, it becomes a column operation.

* Determinant
:PROPERTIES:
:ID:       f8c95c88-93e0-4d50-8e0c-6ef45ece0511
:END:
- A property of a [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] \(A\) with entries (a_{i,j})
- Let \(M_{ij}\) be the matrix generated by removing the ith row and jth column from \(A\)
  - Then we have \(det(A) = \begin{cases}a_{1,1}&\text{if } n = 1 \\ a_{1,1}A_{1,1}+a_{1,2}A_{1,2} + ... + a_{1,n}A_{1,n} &\text{if } n > 1\end{cases}\)
    - where \(A_{i,j} = (-1)^{i+j}det(M_{i,j})\), known as the (i,j) cofactor of \(A\)
** Determinant of a 2x2 matrix
- \(\begin{bmatrix}a&b\\c&d\end{bmatrix}\)
- \(ad-bc\)
** Geometrical Interpretation
- [[file:media/determinant-geometry-2d_1.png][Parallelogram]]
- [[file:media/determinant-geometry-3d_1.png][Parallelepiped]]

** Cofactor Expansions
:PROPERTIES:
:ID:       a79b27cd-982a-4846-9c5e-257caf4f17e9
:END:
- For any \(i in 1...n, j in 1...n\),
  - \(det(A) =  a_{i,1}A_{i,1}+a_{i,2}A_{i,2} + ... + a_{i,n}A_{i,n}\)
  - \(det(A) = a_{1,i}A_{1,i}+a_{2,i}A_{2,i} + ... + a_{n,j}A_{n,j}\)
- In other words, you can use any row or column to find the determinant of a matrix
** Determinant of a [[id:c4062e3e-4992-4d7f-8621-984120945600][Transpose of a Matrix]]
:PROPERTIES:
:ID:       dfea781a-3f39-4b65-befe-9851ae0c141e
:END:
 - \(det(A) = det(A^T)\)
** Determinant of a Triangular Matrix
:PROPERTIES:
:ID:       dbd29807-554b-4169-a53d-ac70674f15bc
:END:
- For [[id:42792e44-1879-489f-b58e-32f01fec557f][Upper Triangular Matrix]] and [[id:c26f10dd-0798-45eb-be5f-837e5aad51bd][Lower Triangular Matrix]], the determinant is the product of the diagonal elements.
** Determinant of Special Matrices
:PROPERTIES:
:ID:       e01768ae-a402-455e-b932-479297e23f46
:END:
- If there are two identical rows or two identical columns, then the determinant is 0.

* Effect of [[id:bf26f901-fa17-4c98-b8bf-8d54d5a8f01d][Elementary Row Operations]] on the [[id:f8c95c88-93e0-4d50-8e0c-6ef45ece0511][Determinant]]
:PROPERTIES:
:ID:       33a7d7a7-a693-464d-a0a7-340053c882e9
:END:
- Multiplying a row by k multiplies the determinant by k
  - In this case, E would be something like \(\begin{bmatrix}1&0\\0&k\end{bmatrix}\), where \(B=EA\)
  - Then, we have \(det(E)det(A)=k \times det(A) = det(B) = det(EA)\)
- Swapping row i and row j multiplies the determinant by -1
  - In this case, E would be something like \(\begin{bmatrix}0&1\\1&0\end{bmatrix}\), where \(B=EA\)
  - Then, we have \(det(E)det(A)=-1 \times det(A) = det(B) = det(EA)\)
- Adding k times row j to row i does not change the determinant
  - In this case, E would be something like \(\begin{bmatrix}1&k\\0&1\end{bmatrix}\), where \(B=EA\)
  - Then, we have \(det(E)det(A)=1 \times det(A) = det(B) = det(EA)\)

** Finding the Determinant Easily By Hand
- Use Gaussian Elimination to make the matrix triangular, then use [[id:dbd29807-554b-4169-a53d-ac70674f15bc][Determinant of a Triangular Matrix]] to find the determinant of the [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]] of the matrix
- Use the above transformations in relation to the determinant to determine the [[id:f8c95c88-93e0-4d50-8e0c-6ef45ece0511][Determinant]] of the original matrix
* [[id:f8c95c88-93e0-4d50-8e0c-6ef45ece0511][Determinant]] Properties
- If c is a scalar and A is a n by n square matrix, then \(det(cA)=c^n \times det(A)\)
  - Simply proven by writing as applying n row operations
- \(det(AB) = det(A) \times det(B)\)
  - Simply proven when A is invertible, just write it as product of elementary matrices. If A is not, then the determinant is 0
- \(det(A^{-1})=\frac{1}{det(A)}\)
  - Simply proven by writing A as the product of elementary matrices
* Adjoints
:PROPERTIES:
:ID:       cf429507-5ed4-4d38-a3e5-f5cbf0075f2f
:END:
- Let A be a square matrix of order n
- \(adj(A)\) is found by replacing all elements i,j with the i,j-cofactor of A, then taking the tranpose
- \(A^{-1}=\frac{1}{det(A)} \times adj(A)\)
* Cramer's Rule
:PROPERTIES:
:ID:       90e7cee6-a91a-4c16-89b5-9d88b3f0ab3b
:END:
- If \(Ax=B\), and \(A_i\) is the matrix obtained by replacing the ith column of \(A\) by \(B\)
- And, If \(A\) is invertible, \(x=\frac{1}{det(A)}\begin{bmatrix}det(A_1)\\det(A_2)\\det(A_3)\\...\\det(A_n)\end{bmatrix}\)
* Vectors
:PROPERTIES:
:ID:       33bef66c-f30e-461c-92c0-45d75d742445
:END:
- Geometric Representation
  - A vector is a line segement with a direction and a length (magnitude)
  - The exception is the zero vector, which has no direction
  - Two vectors are considered equivalent if their length and direction are equal
  - Vectors can be summed by attaching one to the end of the other, and completing the triangle
    - Addition is commutative
  - The negative of a vector is obtained by reversing the direction
  - The scalar multiple of a vector is obtained by multiplying the length (and reversing direction if the scalar is negative)
- Algebraic Representation
  - Use a N-Dimentional coordinate system
  - If a vector has an initial point at the origin, we represent the vector as the other endpoint of V
  - Adding two vectors is just adding the coordinates
  - Multiplying vectors by a scalar is just multiplying the coordinates by that scalar
** N-Vectors
:PROPERTIES:
:ID:       dc6fb874-3fa2-4f2a-86f5-108546757cff
:END:
- A vector with N components
- \((u_1, u_2, ..., u_N)\), where u_k are all real (at least for MA2001)
- Can be represented either as a 1 by n matrix as a "row vector" or a n by 1 matrix as a "column vector"
  - Should not be mixed within the same context
** Euclidean N-Space
:PROPERTIES:
:ID:       5860489e-a49c-4eb2-be85-e4a4ae6c3f60
:END:
- The set of all [[id:dc6fb874-3fa2-4f2a-86f5-108546757cff][N-Vectors]] with real number coordinates for some N
** Vector Sets
- Subsets are inplicitly denoted as \(S={\text{form} | \text{conditions}}\), for example \(S={(0, a, b, a) | a \in {3,4}, b \in {4,5}}\)
- Can also be explicitly denoted like similarly to the general solutions to [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][Linear System]]
- Finite sets have a cardinality aka how many are in the set, denoted by \(|S|\)
* Linear Combinations
:PROPERTIES:
:ID:       1e829ed7-c312-4549-95de-3a4b11156f87
:END:
- Given a set of k [[id:33bef66c-f30e-461c-92c0-45d75d742445][Vectors]], having k constants \(c_1, c_2, ..., c_k\) gives you the linear combination \(c_1 v_1 + c_2 v_2 + ... + c_k v_k\)
- Related to [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][Linear Systems]]
* Linear Spans
:PROPERTIES:
:ID:       6fb2cd00-5a7a-48c6-baa8-89cc39579c6a
:END:
- Given a set of k [[id:33bef66c-f30e-461c-92c0-45d75d742445][Vectors]], the linear span is the set of all [[id:1e829ed7-c312-4549-95de-3a4b11156f87][Linear Combinations]] of those vectors is the linear span of those vectors
- The span takes up the whole space (\(span(S) = \mathbb{R}^n\)) iff the equation \(c_1u_1 + c_2u_2 + ... + c_ku_k = v\) has a solution for any v
- In other words if \(A = \begin{bmatrix}u_1 & u_2 & u_3 & \dots & u_k\end{bmatrix}\):
  - If the [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]] of A has no zero rows, then the linear span takes up the whole space
  - If the REF of A has at least one zero row, then the system is not always consistent, and thus the span does not take up the whole space.
- Let \(S\) be a set of k vectors
  - If \(k<n\), then \(span(S) \neq \mathbb{R}^n\)
- Properties
  - \(0 \in span(S)\)
  - Span is closed under linear combinations (For any \(v_1, v_2, ..., v_r \in span(S)\), \(c_1v_1 + c_2v_2 + ... + c_rv_r \in span(S)\))
  - \(span(S_1) \subseteq span(S_2)\) iff each element in \(S_1\) satisfies \(u_i \in span(S_2)\), a.k.a. all \(u_i\) is a linear combination of \(v_1, v_2, ... v_l\)
    - \(S_1 = \{u_1, u_2, ..., u_k\}\), \(S_2 = \{v_1, v_2, ..., v_l\}\)
** Redundant Vector
:PROPERTIES:
:ID:       c23ade51-65bc-4de5-89ed-d628f2e2e773
:END:
- Let \(S = \{u_1, u_2, ..., u_k\}\)
- If \(u_k\) is a linear combination of \(\{u_1, u_2, ..., u_{k-1}\}\), then \(span(\{u_1, u_2, ..., u_k\}) = span(\{u_1, u_2, ..., u_{k-1}\})\)
** Geometrical Interpretations
:PROPERTIES:
:ID:       714e83f9-5fd7-44f8-9a5a-0d04a2d61aa0
:END:
- If we have a single vector in 2D, the span is a line through the origin parallel to that vector
- If we have two vectors in 2D:
  - If they are parallel, one of them is a [[id:c23ade51-65bc-4de5-89ed-d628f2e2e773][Redundant Vector]], and so it is also a line through the origin parallel to the vectors
  - Otherwise, it encompasses the whole space
- If we have two vectors in 2D:
  - Parallel, same thing
  - Otherwise, it encompasses the plane that contains the two vectors (when set to the origin) and the origin
- [[file:media/line_1.png][Line]]
- [[file:media/plane_1.png][Plane]]
*** k-Plane
:PROPERTIES:
:ID:       6144f960-7456-44e7-ae72-68a444e73cf4
:END:
- In n-dimensions
- If we have \(x, u_1, u_2, ..., u_r \in \mathbb{R}^n\)
- The set \(Q= \{x+w | w \in span(\{u_1, u_2, ..., u_r\})\}\) is a k-plane in \(\mathbb{R}^n\), where k is the dimension of the span
* Subspaces
:PROPERTIES:
:ID:       b91e4b67-de10-40c3-bea8-b950d195156e
:END:
- From [[id:6fb2cd00-5a7a-48c6-baa8-89cc39579c6a][Linear Spans]], we know that the linear span of, for example, a plane in a 3D space can act the same as the \(\mathbb{R}^2\) space
- More rigidly, a subset \(V \subset \mathbb{R}^n\) is a subspace of \(\mathbb{R}^n\) if there exist \(S=\{u_1, u_2, ..., u_k \in \mathbb{R}^n\}\) such that \(V = span(S)\)
  - In this case we say \(V\) is a subspace spanned by \(S\)
- Alternatively: a non empty set \(V\) is a subspace iff for all \(u, v \in V\), \(c, d \in \mathbb{R}\) \(cu + dv \in V\)
- Trivial subspaces:
  - \(span(\{0\})\), or the zero subspace.
  - \(\mathbb{R}^n\), which itself is also a subspace (\(span(\{(1, 0, ...), (0, 1, ...),..., (0, 0, ..., 1)\}\))
- Subspaces of \(\mathbb{R}^2\)
  - The origin
  - Any line through the origin
  - The whole space
- Subspaces of \(\mathbb{R}^3\)
  - The origin
  - Any line through the origin
  - Any plane through the origin
  - The whole space
** Proving Subspaces
:PROPERTIES:
:ID:       bf2956d1-0da6-47f9-bbd7-adc69657097f
:END:
- To prove a subset is not a subspace, we simply need a counterexample.
- We can use some of these properties:
  - \(0 \in V\)
  - For any \(v_1, v_2, ..., v_r \in V\), and any \(c_1, c_2, ..., c_r \in \mathbb{R}\), \(c_1v_1 + c_2v_2 + ... + c_rv_r \in V\)
- Use the alternate definition of subspace
** Solution Spaces
:PROPERTIES:
:ID:       c99030a7-f2fe-408d-ae2b-2fae64b1c1df
:END:
 - The [[id:6d6c3bfc-2b01-4012-b255-d155b17fdc04][Solution Set]] of a [[id:98073aba-eb13-435c-b020-535c499f2767][Homogenous Linear System]] in \(n\) variables is a subspace of \(\mathbb{R}^n\)
 - The [[id:5ba9a192-afee-4100-9f47-f6a4f81b57de][Dimension]] of that space will be equivalent to the number of non-pivot columns, AKA the number of arbitrary parameters
** Subspace of a Subspace
:PROPERTIES:
:ID:       e5770fe4-53cc-4f1c-81e1-411a01a2898f
:END:
- If we have a subspace \(V\) of \(\mathbb{R}^n\), then a set \(W\) is a subspace of \(V\) if it s a subspace of \(\mathbb{R}^n\) and \(W \subseteq V\)
* Linear Independence
:PROPERTIES:
:ID:       b11750a1-3628-44c3-bf17-bedc80748f5e
:END:
- Let \(S = \{u_1, u_2, ..., u_k\} \in \mathbb{R}^n\)
- Consider the equation \(c_1u_1+c_2u_2+...+c_ku_k = 0\) with c as variables (this is a [[id:98073aba-eb13-435c-b020-535c499f2767][Homogenous Linear System]])
- Iff the trivial solution is the only solution, \(S\) is said to be linearly independent
  - By this definition, a set of \(n+1\) or more vectors in \(n-dimensional\) space must be linearly dependent
- Otherwise, \(S\) can be said to be linearly dependent
- Alternatively, we have \(S\) is linearly independent iff no vector in \(S\) is a linear combination of the other vectors in \(S\)
- A [[id:6fb2cd00-5a7a-48c6-baa8-89cc39579c6a][Linear Span]] of \(S\) has no redundant vectors iff \(S\) is linearly independent
- Given a set of linearly independent vectors, adding a vector that is not contained in the span of that original set will maintain the independence of that set.

* Vector Space
:PROPERTIES:
:ID:       b3caafea-8006-4974-b41a-13c2917d32c9
:ROAM_ALIASES: Basis
:END:
- A set that is either \(\mathbb{R}^n\) or a [[id:b91e4b67-de10-40c3-bea8-b950d195156e][Subspace]] of it.
* Bases
:PROPERTIES:
:ID:       f061a0cd-618a-4339-8982-7e1c425463a9
:END:
- Let \(V\) be a [[id:b3caafea-8006-4974-b41a-13c2917d32c9][Vector Space]], and let \(S\) be a subset of \(V\).
- \(S\) is a basis of \(V\) iff
  - \(S\) is linearly independent
  - \(S\) spans \(V\), i.e. \(span(S) = V\)
- By this definition, \(S\) is the smallest set that can span \(V\)
- For convenience, the empty set is the basis of the zero space.
- For all other spaces, every vector space has infinitely many bases.
** Using Bases as Coordinate Systems
:PROPERTIES:
:ID:       f6435027-149e-42ef-b351-39340571954f
:END:
- Given a vector \(v \in V\), there exists unique \(c_1, c_2, ..., c_k\) such that \(v=c_1u_1 + c_2u_2 + ... + c_ku_k\) if \(S=\{u_1, u_2, ..., u_k\}\) is a base of \(V\)
- We can say that \(c_1, c_2, ..., c_k\) are the coordinates of \(v\) relative to \(S\)
- Note: the order of vectors in the basis matters, so S is actually an ordered set of vectors.
- Notation:
  - \((v)_s=\begin{pmatrix}c_1 & c_2 & c_3 & ... & c_k\end{pmatrix}\)
  - \([v]_s=\begin{pmatrix}c_1 \\ c_2 \\ c_3 \\ \vdots \\ c_k\end{pmatrix}\)
- Properties:
  - For any \(u, v\): \(u = v\) iff \((u)_s = (v)_s\)
  - For any \(v_1, v_2, ..., v_k\) and \(c_1, c_2, ..., c_k\): \((c_1v_1 + c_2v_2 + ... + c_kv_k)_s = c_1(v_1)_s + c_2(v_2)_s + ... + c_k(v_k)_s\)
  - Given a basis \(S\) of cardinality k:
    - Given a set of vectors in \(V\), they are linearly independent iff their coordinates in \(S\) are linearly independent
    - The span of that set is equal to \(V\) iff the span of their coordinates in \(S\) is equal to \(\mathbb{R}^k\)
** Standard Basis
:PROPERTIES:
:ID:       a05c5eec-5069-4633-ada8-181efca0c408
:END:
- The standard basis for \(\mathbb{R}^n\) is \(E={e_1, e_2, e_3, ..., e_n}\) where \(e_1=(1, 0, 0, ...), e_2=(0, 1, 0, ...), e_3=(0,0,1,..), ...\)
** Dimension
:PROPERTIES:
:ID:       5ba9a192-afee-4100-9f47-f6a4f81b57de
:END:
- For a [[id:b91e4b67-de10-40c3-bea8-b950d195156e][Subspace]] with a basis with cardinality k:
  - Any subset with more than k vectors are linearly dependent
  - All bases have cardinality k
  - Any subset with less than k vectors cannot span the subspace.
- The dimension of a vector \(V\), \(dim(V)\), is equal to the cardinality of a base of \(V\).
  - The dimension of the zero space is 0.
- For solution spaces:
  - Write the solution \(x = t_1u_1 + t_2u_2 + t_3u_3 + ... + t_ku_k\), where \(t_i\) are parameters and \(u_i\) are vectors.
  - Then, $u_1, ..., u_k$ is a basis of the solution space.
  - Thus the dimension of the solution space is equal to $k$.
- If you know the dimension of \(V\) is \(k\), the following are equivalent:
  - S is a basis for \(V\)
  - S is linearly independent and \(|S| = k\)
  - S spans \(V\) and \(|S|=k\)
- If \(W\) is a subspace of \(V\), then \(dim(W) \leq dim(V)\)
-
* Transition Matrices
:PROPERTIES:
:ID:       afa55c4d-3f3e-4b23-84f8-29bc57319011
:END:
 - Let two sets \(S = \{u_1, u_2, ..., u_k\}\) and \(T=\{v_1, v_2, ..., v_k\}\) be [[id:f061a0cd-618a-4339-8982-7e1c425463a9][Bases]] of \(V\)
 - \(u_i\) can be written as a linear combination of \(T\)
 - Given a vector \(W\), how to go from \([W]_S\) to \([W]_T\)
   - \([W]_T=\begin{bmatrix}[u_1]_T & [u_2]_T & ... & [u_k]_T\end{bmatrix}[W]_S\)
   - Let \(P = \begin{bmatrix}[u_1]_T & [u_2]_T & ... & [u_k]_T\end{bmatrix}\) then, \([W]_T=P[W]_S\)
   - P is called the transition matrix from \(S\) to \(T\)
   - Properties:
     - \(P\) is invertible
     - \(P^{-1}\) is the transition matrix from \(T\) to \(S\)
* Vector Spaces Associated with Matrices
:PROPERTIES:
:ID:       1a30541d-aaf1-41c4-9d1f-edee848bd15e
:END:
** Row Spaces
:PROPERTIES:
:ID:       a8143ca8-980f-4ea3-9ed7-41f67b339390
:END:
- Let \(A\) be an m by n matrix \(\begin{bmatrix}r_1 \\ \vdots \\ r_m \end{bmatrix}\)
- The Row Space of \(A\) is the subspace of of \(\mathbb{R}^n\) spanned by the rows of \(A\)
- [[id:03fd9b35-48e8-42da-a4bf-bb6942228e03][Row Equivalent]] matrices have the same row space
  - [[id:bf26f901-fa17-4c98-b8bf-8d54d5a8f01d][Elementary Row Operations]] preserve the row space
  - The non-zero rows in the [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]] of \(A\) are linearly independent and therefore form the basis of the row space of \(A\)
** Column Spaces
:PROPERTIES:
:ID:       2e88699a-cfb2-48a0-965f-1ad484989a4c
:END:
- Let \(A\) me an m by n matrix \(\begin{bmatrix}c_1 & ... & c_n\end{bmatrix}\)
- The Column Space of \(A\) is the subspace of \(\mathbb{R}^m\) spanned by the columns of \(A\)
- [[id:03fd9b35-48e8-42da-a4bf-bb6942228e03][Row Equivalent]] matrices may not have the same row space
  - However, if we have two row equivalent matrices \(S\) and \(T\), the set of columns in \(A\) given by the column numbers \(c_1, c_2, ..., c_k\) are linearly independent iff the columns in \(B\) given by the column numbers \(c_1, c_2, ..., c_k\) are linearly independent
  - [[file:media/column-vectors_1.png][Diagram]]
  - A result of this is that the basis for the column space of \(A\) can be found by taking the columns corresponding to the pivot columns in the [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]] of \(A\)
** Linear Systems
:PROPERTIES:
:ID:       31cdb48d-e5a5-4e09-b926-0075a1e2c7ce
:END:
- The column space of \(A\) (m by n) is \(\{Au | u \in \mathbb{R}^n\}\)
- \(Ax=b\) is consistent iff b is in the column space of \(A\)
- In other words, solutions to \(Ax=b\) are finding ways to write \(b\) as a linear combination of the columns of \(A\)
* Rank of a [[id:d7a14523-8aca-462b-96d7-96f8c198e3e9][Matrix]]
:PROPERTIES:
:ID:       ff7ecda7-522c-49e8-9479-7880299efb03
:END:
- The [[id:a8143ca8-980f-4ea3-9ed7-41f67b339390][Row Spaces]] and [[id:2e88699a-cfb2-48a0-965f-1ad484989a4c][Column Spaces]] of a matrix have the same [[id:5ba9a192-afee-4100-9f47-f6a4f81b57de][Dimension]] (even though they may be subspaces of a different real space)
  - This is just a result of the number of pivot columns being equal to the number of non-zero rows
  - The [[id:2e30e6b2-9251-4c46-996d-f73cc7a14329][Pivot Columns]] form a basis for the column space
  - The non-zero rows form a basis for the row space
- The rank of a matrix is the dimension of the row space, equal to the dimension of the column space.
- Denoted by \(rank(A)\)
- Properties:
  - \(rank(0)=0\) and \(rank(I_n)=n\)
  - For a m by n matrix, \(rank(A) \leq min(m,n)\)
    - A matrix with \(rank(A)=min(m,n)\) is said to be full rank.
    - A [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] is only full rank iff it is [[id:5c656b0b-ab52-4d87-89b7-268c6aa6661c][Invertible]]
  - \(rank(A) = rank(A^T)\)
  - A [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][Linear System]] \(Ax=b\) is [[id:b2f4051d-ee59-40dc-a561-4ea4f14fb79f][Consistant]] iff \(A\) and \((A|b)\) have the same rank
  - \(rank(AB) \leq min(rank(A), rank(B))\)

* Nullspaces
:PROPERTIES:
:ID:       e8099a6c-6a63-4a1e-a196-bd60568b8c9b
:END:
- Let A be a m by n matrix
- The solution space of the [[id:98073aba-eb13-435c-b020-535c499f2767][Homogenous Linear System]] \(Ax=0\) is known as the nullspace of \(A\)
- The [[id:5ba9a192-afee-4100-9f47-f6a4f81b57de][Dimension]] of the null space is called the nullity of \(A\), denoted as \(nullity(A)\)
- Since the null space is a subspace of \(\mathbb{R}^n\), we have \(nullity(A) \leq n\)
- By convension, vectors in nullspaces and solutions to linear systems will be written as column vectors.
- Nullity is equal to the number of non-pivot columns in the [[id:b0ddc803-3424-4cd1-b60e-4311c41a6098][REF]] (the number of arbitrary parameters)
  - \(rank(A) + nullity(A)\) is equal to the number of columns in \(A\)
- Implication on [[id:4de0b32f-feb4-42e9-8bca-194ed2408758][Linear Systems]]
  - If we have a solution \(x=v\) to the system \(Ax=b\)
  - Then, the solution set of the system is given by \(\{u+v|u \in \text{nullspace of A}\}\)
  - [[file:media/differential_1.png][Related]]

* Orthogonality
:PROPERTIES:
:ID:       da0cec81-2f89-4f95-990d-1146a0f9b1bf
:ROAM_ALIASES: Orthogonal
:END:
- Two vectors \(u and v\) are called orthogonal if \(u \cdot v = 0\)
  - Implies that the angle between them is \(\frac{\pi}{2}\)
- A set of vectors in \(\mathbb{R}^n\) is called an orthogonal set if every pair of distinct vectors in \(S\) are orthogonal
  - Such a set that is also a basis of a vector space is an orthogonal basis
- The set is orthonormal if it is orthogonal and every vector is a unit vector, i.e. of length 1
  - Such a set that is also a basis of a vector space is an orthonormal basis
- Properties:
  - An orthogonal set of nonzero vectors is linearly independent
** Orthogonal Bases
:PROPERTIES:
:ID:       abdbe67d-ce90-4dab-8bae-ce9f1770de5a
:END:
 - If a set \(S = \{u_1, u_2, ..., u_k\}\) which is an orthogonal basis for a vector space \(V\)
 - Then, for any \(w \in V\), we have \(w = \frac{w \cdot u_1}{u_1 \cdot u_1}u_1 + ... + \frac{w \cdot u_k}{u_k \cdot u_k}u_k\)
 - If \(S\) is in fact an orthonormal basis for \(V\), then \(w=(w \cdot u_1)u_1 + ... + (w \cdot u_k)u_k\)
** Orthogonal to a Vector Space
:PROPERTIES:
:ID:       f06b7d74-72cd-4e77-94f9-a2decdc71903
:END:
- A vector \(u \in \mathbb{R}^n\) is orthogonal to a vector space \(V\) if \(u\) is orthogonal to all vectors in \(V\)
- If \(V = \{v_1, v_2, ..., v_k\}\) then \(u\) is orthogonal to \(V\) iff
  \(u \cdot v_i = 0\) for all i.
* Projection
:PROPERTIES:
:ID:       741e22cd-b761-4d94-b74b-b1fdfbff9a4a
:END:
- Every \(u \in \mathbb{R}^n\) can be written uniquely as \(u = n + p\) where \(p \in V\) and \(n\) is [[id:f06b7d74-72cd-4e77-94f9-a2decdc71903][Orthogonal]] to \(V\)
  - \(p\) is called the projection of \(u\) onto \(V\)
- We can find the projection of \(w\) in a subspace with [[id:abdbe67d-ce90-4dab-8bae-ce9f1770de5a][Orthogonal Basis]] \(\{u_1, u_2, ..., u_k\}\): \(\frac{w \cdot u_1}{u_1 \cdot u_1}u_1 + ... + \frac{w \cdot u_k}{u_k \cdot u_k}u_k\)
- Similarly, if \(\{u_1, u_2, ..., u_k\}\) is an Orthonormal Basis:
  \((w \cdot v_1)v_1 + ... +(w \cdot v_k)v_k\)
** Converting a [[id:b3caafea-8006-4974-b41a-13c2917d32c9][Basis]] to a [[id:abdbe67d-ce90-4dab-8bae-ce9f1770de5a][Orthogonal Basis]]
:PROPERTIES:
:ID:       9f028eca-19a7-404b-8ae2-64318aacd4eb
:ROAM_ALIASES: "Grant-Schmidt Process"
:END:
- Given the basis \(\{u_1, u_2\}\), find the projection \(p\) of \(u_2\) onto \(span(u_1)\)
- Thus, we get the orthogonal basis \(\{u_1, u_2 - p\}\)
- This process can be extended to more dimensions by recursing (from 3 dimensions you take out one vector first, apply this process to achieve a orthogonal basis for the remaining two, then add back in the last vector the same way we added in u_2)
- Grant-Schmidt Process:
  - Let \(v_1 = u_1\)
  - \(v_2 = u_2 - \frac{u_2 \cdot v_1}{v_1 \cdot v_1}v_1\)
  - \(v_3 = u_3 - \frac{u_3 \cdot v_1}{v_1 \cdot v_1}v_1 - \frac{u_3 \cdot v_2}{v_2 \cdot v_2} v_2\)
  - And so on...
  - Then, \(\{v_1, ..., v_k\}\) is an orthogonal basis for the vector space where \(\{u_1, ..., u_k\}\) is a basis.
- To extend to an orthonormal basis, simply normalize each vector (divide by length)
* Length of Vectors
:PROPERTIES:
:ID:       e43d3f89-f22a-4326-b6c6-7996e22444a7
:END:
- In 2 dimensions, the vector \(u=(u_1, u_2)\) has length/norm/euclidean norm \(||u|| = \sqrt{u_1^2+u_2^2}\)
- This extends to an arbitrary number of dimensions: \(v=(v_1, v_2, ..., v_k)\) has length \(||v||=\sqrt{v_1^2+v_2^2+...+v_k^2}\)
* Distance and Angles Between Vectors
:PROPERTIES:
:ID:       f2728d2e-3632-492b-91fb-bb9b232b8480
:END:
- Regarding vectors in 2 and 3 dimensions:
  - The distance between two vectors \(d(u,v) = ||u - v||\)
  - The cosine rule of trig states that \(||u-v||^2=||u||^2+||v||^2-2||u||||v|| cos(\theta)\)
    - When solving for angle, take the principal value, between \(0,\pi\)
- For vectors in more dimensions:
  - Same formulas apply
* Dot Product
:PROPERTIES:
:ID:       3a15c525-094e-4241-bbe0-3b3007c43009
:END:
- The dot product of \(u = (u_1, u_2, ..., u_n)\) and \(v = (v_1, v_2, ..., v_n)\), expressed as \(u \cdot v\), is defined as \(u_1v_1 + u_2v_2 + ... + u_nv_n\)
- We get \(||u|| = \sqrt{u \cdot u}\)
- Distance between u and v is \(||u-v|| = \sqrt{(u-v) \cdot (u-v)}\)
- Angle between u and v is \(cos^{-1}(\frac{u \cdot v}{||u|| ||v||})\)
- If we express u and v as row vectors, then \(u \cdot v = uv^{T}\)
- If column, then \(u \cdot v = u^{T}v\)
- Properties:
  - \(u \cdot v = v \cdot u\)
  - \((u+v) \cdot w = u \cdot w + v \cdot w\)
  - \(||cu|| = |c|||u||\)
  - \((cu) \cdot v = u \cdot (cv) = c(u \cdot v)\)
  - \(u \cdot u \geq 0\)
    - \(u \cdot u = 0\) iff \(u = 0\)
* Approximations
:PROPERTIES:
:ID:       dd085513-d24b-48c3-b722-7a23d2488cf5
:END:
- Given a subspace V and a vector u with its projection p onto V, the distance between \(u\) and any other vector in \(V\) is greater than or equal to the distance between \(u\) and \(p\)
- Given a linear system representing the results of an experiment (for example, calculating the orbits of planets), it will generally be inconsistent due to errors in measurements
- If we want to minimize the sum of squares of errors, this can actually be written as minimizing the norm of the difference \(b - Ax\)
  - In other words, a vector \(u\) is the least-square solution of \(Ax=b\) if \(||b-Au|| \leq ||b-Av||\) for all \(v \in \mathbb{R}^n\)
- \(Au\) is the projection of \(b\) onto the column space of \(A\)
  - After finding \(Au\), you can simply solve for \(u\)
- Without projecting:
  - \(u\) is the least-squared solution iff \(u\) must is a solution to \(A^TAx=A^Tb\)
* Orthogonal Matrix
:PROPERTIES:
:ID:       65196b2f-c31a-4ae6-a61c-c862f205c5ce
:END:
- The following are equivalent:
  - \(P\) is an orthogonal matrix
  - \(P\) is a [[id:afa55c4d-3f3e-4b23-84f8-29bc57319011][Transition Matrix]] between two [[id:abdbe67d-ce90-4dab-8bae-ce9f1770de5a][Orthonormal Bases]]
  - The columns of \(P\) form another orthonormal basis
  - The rows of \(P\) form another orthonormal basis
  - The inverse of \(P\) is \(P^T\)
    - Can check that \(PP^T=I\)
- An example:
  - The rotation matrix \(\begin{bmatrix}cos(\theta)&-sin(\theta)\\sin(\theta)&cos(\theta)\end{bmatrix}\)
* Diagonalization
:PROPERTIES:
:ID:       52d4826f-7ab6-4de2-be9f-b76b12f2be46
:END:
- To find \(A^n\) as \(n \to \infty\)
- Find a [[id:60269c0a-1614-4f10-bb35-6831c702d103][Diagonal Matrix]] \(D\) and another matrix \(P\) such that \(PDP^{-1}=A\)
- Then, \(A^n = PD^nP^{-1}\)
  - \(D^{n}\) is easy to compute because it is diagonal
  - This makes the limit easy to compute
- How do you find \(P\) and \(D\)? When is it possible?
- A [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] \(A\) is diagonalizable if there exists an invertible matrix \(P\) such that \(P^{-1}AP\) is a diagonal matrix
  - We say that \(P\) diagonalizes \(A\)
- For \(A\) of order n, \(A\) is diagonalizable iff \(A\) has \(n\) linearly independent [[id:9098094c-3584-417f-bece-3218f546e590][Eigenvectors]]
- How to diagonalize a matrix:
  - Find all distinct [[id:9098094c-3584-417f-bece-3218f546e590][Eigenvalues]]
  - For each \(\lambda_i\), find the basis \(S_{\lambda_i}\) of the [[id:dfe13ffb-fd35-4c05-8e8e-687490e85d24][Eigenspace]] \(E_{\lambda_i}\)
    - Not all of them may be real, in which case the algorithm works but some entries may be complex
  - Take \(S = S_{\lambda_1} \cup S_{\lambda_2} \cup ... \cup S_{\lambda_k}\)
    - \(S\) is guaranteed to be linearly independent
  - If \(S\) has less than \(n\) elements, then \(A\) is not diagonalizable.
  - If \(S\) has exactly \(n\) elements, then \(A\) is diagonalizable.
    - Let \(S = \{u_1, u_2, ..., u_n\}\)
    - Then \(P=\begin{bmatrix}u_1 & u_2 & ... & u_n\end{bmatrix}\) diagonalizes \(A\)
* Eigenvectors
:PROPERTIES:
:ID:       9098094c-3584-417f-bece-3218f546e590
:ROAM_ALIASES: Eigenvalues
:END:
- A nonzero column vector is an eigenvector of a n by n [[id:390d4ad5-a285-441e-a435-b613c1ad4190][Square Matrix]] \(A\) if \(Au = \lambda u\) for some scalar \(\lambda\)
  - "As far as multiplying to u is concerned, A acts as if it was a [[id:60269c0a-1614-4f10-bb35-6831c702d103][Diagonal Matrix]]"
- The scalar \(\lambda\) is an eigenvalue of \(A\), and \(u\) is the eigenvector associated with the eigenvalue \(\lambda\)
- The following are equivalent:
  - \(\lambda\) is an eigenvalue of \(A\)
  - \(Au = \lambda u\) for some nonzero column vector \(u\)
  - \(\lambda I - A)u=0\) for some nonzero column vector \(u\)
  - The linear system \((\lambda I - A)x = 0\) has non-trivial solutions
  - \(det(\lambda I - A) = 0\)
    - If expanded, we get a polynomial of degree \(n\)
- For a [[id:c26f10dd-0798-45eb-be5f-837e5aad51bd][Triangular Matrix]], the eigenvalues are simply the values on the diagonal.
  - Note: Row operations do not preserve eigenvalues, so we cannot manipulate the matrix into REF to abuse this fact.
** Characteristic Equation and Polynomial
:PROPERTIES:
:ID:       532a0f4b-b4b6-47b3-a8d5-6bd18d44effd
:END:
- The equation \(det(\lambda I - A) = 0\) is the characteristic equation of \(A\)
- The polynomial \(det(\lambda I - A)\) is the characteristic polynomial of \(A\)
* Eigenspaces
:PROPERTIES:
:ID:       dfe13ffb-fd35-4c05-8e8e-687490e85d24
:END:
- An eigenspace is the collection of all \(u\) such that \(Au = \lambda u\)
- In other words, it is the [[id:c99030a7-f2fe-408d-ae2b-2fae64b1c1df][Solution Space]] of the linear system \((\lambda I - A)x = 0\)
- This is denoted as \(E_\lambda(A)\)
- All vectors in the eigenspace are [[id:9098094c-3584-417f-bece-3218f546e590][Eigenvectors]] except for the trivial solution / zero vector
- The dimension of the eigenspace is at most the power of the factor leading to the corresponding eigenvalue as a solution within the [[id:532a0f4b-b4b6-47b3-a8d5-6bd18d44effd][Characteristic Equation]]
